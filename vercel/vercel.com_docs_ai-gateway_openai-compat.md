---
url: "https://vercel.com/docs/ai-gateway/openai-compat"
title: "OpenAI-Compatible API"
---

- Products


- ##### [AI Cloud](https://vercel.com/ai)


  - [AI SDK\\
    \\
    The AI Toolkit for TypeScript](https://sdk.vercel.ai/)
  - [AI Gateway\\
    \\
    One endpoint, all your models](https://vercel.com/ai-gateway)
  - [Vercel Agent\\
    \\
    An agent that knows your stack](https://vercel.com/agent)
- ##### Core Platform


  - [CI/CD\\
    \\
    Helping teams ship 6× faster](https://vercel.com/products/previews)
  - [Content Delivery\\
    \\
    Fast, scalable, and reliable](https://vercel.com/products/rendering)
  - [Fluid Compute\\
    \\
    Servers, in serverless form](https://vercel.com/fluid)
  - [Observability\\
    \\
    Trace every step](https://vercel.com/products/observability)
- ##### [Security](https://vercel.com/security)


  - [Bot Management\\
    \\
    Scalable bot protection](https://vercel.com/security/bot-management)
  - [BotID\\
    \\
    Invisible CAPTCHA](https://vercel.com/botid)
  - [Platform Security\\
    \\
    DDoS Protection, Firewall](https://vercel.com/security)
  - [Web Application Firewall\\
    \\
    Granular, custom protection](https://vercel.com/security/web-application-firewall)

- Resources


- ##### Company


  - [Customers\\
    \\
    Trusted by the best teams](https://vercel.com/customers)
  - [Blog\\
    \\
    The latest posts and changes](https://vercel.com/blog)
  - [Changelog\\
    \\
    See what shipped](https://vercel.com/changelog)
  - [Press\\
    \\
    Read the latest news](https://vercel.com/press)
  - [Events\\
    \\
    Join us at an event](https://vercel.com/events)
- ##### Open Source


  - [Next.js\\
    \\
    The native Next.js platform](https://vercel.com/frameworks/nextjs)
  - [Nuxt\\
    \\
    The progressive web framework](https://nuxt.com/)
  - [Svelte\\
    \\
    The web's efficient UI framework](https://svelte.dev/)
  - [Turborepo\\
    \\
    Speed with Enterprise scale](https://vercel.com/solutions/turborepo)
- ##### Tools


  - [Academy\\
    \\
    Learn the ins and outs of Vercel](https://vercel.com/academy)
  - [Marketplace\\
    \\
    Extend and automate workflows](https://vercel.com/marketplace)
  - [Templates\\
    \\
    Jumpstart app development](https://vercel.com/templates)
  - [Guides\\
    \\
    Find help quickly](https://vercel.com/guides)
  - [Partner Finder\\
    \\
    Get help from solution partners](https://vercel.com/partners/solution-partners)

- Solutions


- ##### Use Cases


  - [AI Apps\\
    \\
    Deploy at the speed of AI](https://vercel.com/solutions/ai-apps)
  - [Composable Commerce\\
    \\
    Power storefronts that convert](https://vercel.com/solutions/composable-commerce)
  - [Marketing Sites\\
    \\
    Launch campaigns fast](https://vercel.com/solutions/marketing-sites)
  - [Multi-tenant Platforms\\
    \\
    Scale apps with one codebase](https://vercel.com/solutions/multi-tenant-saas)
  - [Web Apps\\
    \\
    Ship features, not infrastructure](https://vercel.com/solutions/web-apps)
- ##### Users


  - [Platform Engineers\\
    \\
    Automate away repetition](https://vercel.com/solutions/platform-engineering)
  - [Design Engineers\\
    \\
    Deploy for every idea](https://vercel.com/solutions/design-engineering)

- [Enterprise](https://vercel.com/enterprise)
- [Docs](https://vercel.com/docs)
- [Pricing](https://vercel.com/pricing)

Search DocumentationSearch `⌘ K`

Ask AI

Search DocumentationSearch `⌘ K`

Ask AI

[AI Gateway](https://vercel.com/docs/ai-gateway)

OpenAI-Compatible API

Docs

API Reference

- [Getting Started](https://vercel.com/docs/getting-started-with-vercel)

Expand menu



- [Projects and Deployments](https://vercel.com/docs/getting-started-with-vercel/projects-deployments)

- [Use a Template](https://vercel.com/docs/getting-started-with-vercel/template)

- [Import Existing Project](https://vercel.com/docs/getting-started-with-vercel/import)

- [Add a Domain](https://vercel.com/docs/getting-started-with-vercel/domains)

- [Buy a Domain](https://vercel.com/docs/getting-started-with-vercel/buy-domain)

- [Transfer an Existing Domain](https://vercel.com/docs/getting-started-with-vercel/use-existing)

- [Collaborate](https://vercel.com/docs/getting-started-with-vercel/collaborate)

- [Next Steps](https://vercel.com/docs/getting-started-with-vercel/next-steps)


- [Supported Frameworks](https://vercel.com/docs/frameworks)

Expand menu



- [Full-stack](https://vercel.com/docs/frameworks/full-stack)

Expand menu



- [Next.js](https://vercel.com/docs/frameworks/full-stack/nextjs)

- [SvelteKit](https://vercel.com/docs/frameworks/full-stack/sveltekit)

- [Nuxt](https://vercel.com/docs/frameworks/full-stack/nuxt)

- [Remix](https://vercel.com/docs/frameworks/full-stack/remix)

- [TanStack Start](https://vercel.com/docs/frameworks/full-stack/tanstack-start)


- [Frontends](https://vercel.com/docs/frameworks/frontend)

Expand menu



- [Astro](https://vercel.com/docs/frameworks/frontend/astro)

- [Vite](https://vercel.com/docs/frameworks/frontend/vite)

- [React Router](https://vercel.com/docs/frameworks/frontend/react-router)

- [Create React App](https://vercel.com/docs/frameworks/frontend/create-react-app)


- [Backends](https://vercel.com/docs/frameworks/backend)

Expand menu



- [Nitro](https://vercel.com/docs/frameworks/backend/nitro)

- [Express](https://vercel.com/docs/frameworks/backend/express)

- [Elysia](https://vercel.com/docs/frameworks/backend/elysia)

- [FastAPI](https://vercel.com/docs/frameworks/backend/fastapi)

- [Fastify](https://vercel.com/docs/frameworks/backend/fastify)

- [Flask](https://vercel.com/docs/frameworks/backend/flask)

- [Hono](https://vercel.com/docs/frameworks/backend/hono)

- [NestJS](https://vercel.com/docs/frameworks/backend/nestjs)

- [xmcp](https://vercel.com/docs/frameworks/backend/xmcp)


- [All Frameworks](https://vercel.com/docs/frameworks/more-frameworks)


- [Incremental Migration](https://vercel.com/docs/incremental-migration)

- [Production Checklist](https://vercel.com/docs/production-checklist)

- [Guides](https://vercel.com/guides)

- [Docs llms-full.txt](https://vercel.com/docs/llms-full.txt)


* * *

- Access

Expand menu



- [Account Management](https://vercel.com/docs/accounts)

- [Activity Log](https://vercel.com/docs/activity-log)

- [Deployment Protection](https://vercel.com/docs/deployment-protection)

Expand menu



- [Bypass Deployment Protection](https://vercel.com/docs/deployment-protection/methods-to-bypass-deployment-protection)

Expand menu



- [Exceptions](https://vercel.com/docs/deployment-protection/methods-to-bypass-deployment-protection/deployment-protection-exceptions)

- [OPTIONS Allowlist](https://vercel.com/docs/deployment-protection/methods-to-bypass-deployment-protection/options-allowlist)

- [Protection Bypass for Automation](https://vercel.com/docs/deployment-protection/methods-to-bypass-deployment-protection/protection-bypass-automation)

- [Sharable Links](https://vercel.com/docs/deployment-protection/methods-to-bypass-deployment-protection/sharable-links)


- [Protect Deployments](https://vercel.com/docs/deployment-protection/methods-to-protect-deployments)

Expand menu



- [Password Protection](https://vercel.com/docs/deployment-protection/methods-to-protect-deployments/password-protection)

- [Trusted IPs](https://vercel.com/docs/deployment-protection/methods-to-protect-deployments/trusted-ips)

- [Vercel Authentication](https://vercel.com/docs/deployment-protection/methods-to-protect-deployments/vercel-authentication)


- [Directory Sync](https://vercel.com/docs/directory-sync)

- [SAML SSO](https://vercel.com/docs/saml)

- [Two-factor (2FA)](https://vercel.com/docs/two-factor-authentication)


- AI

Expand menu



- [Vercel Agent](https://vercel.com/docs/agent)

Expand menu



- [Code Review\\
beta](https://vercel.com/docs/agent/pr-review)

Expand menu



- [Managing Reviews](https://vercel.com/docs/agent/pr-review/usage)


- [Investigation\\
beta](https://vercel.com/docs/agent/investigation)

- [Pricing](https://vercel.com/docs/agent/pricing)


- [AI SDK](https://vercel.com/docs/ai-sdk)

- [AI Gateway](https://vercel.com/docs/ai-gateway)

Expand menu



- [Getting Started](https://vercel.com/docs/ai-gateway/getting-started)

- [Models & Providers](https://vercel.com/docs/ai-gateway/models-and-providers)

- [Observability](https://vercel.com/docs/ai-gateway/observability)

- [Pricing](https://vercel.com/docs/ai-gateway/pricing)

- [Provider Options](https://vercel.com/docs/ai-gateway/provider-options)

- [OpenAI-Compatible API](https://vercel.com/docs/ai-gateway/openai-compat)

- [Authentication](https://vercel.com/docs/ai-gateway/authentication)

- [BYOK](https://vercel.com/docs/ai-gateway/byok)

- [Usage & Billing](https://vercel.com/docs/ai-gateway/usage)

- [Framework Integrations](https://vercel.com/docs/ai-gateway/framework-integrations)

Expand menu



- [LangChain](https://vercel.com/docs/ai-gateway/framework-integrations/langchain)

- [LangFuse](https://vercel.com/docs/ai-gateway/framework-integrations/langfuse)

- [LiteLLM](https://vercel.com/docs/ai-gateway/framework-integrations/litellm)

- [LlamaIndex](https://vercel.com/docs/ai-gateway/framework-integrations/llamaindex)

- [Mastra](https://vercel.com/docs/ai-gateway/framework-integrations/mastra)

- [Pydantic AI](https://vercel.com/docs/ai-gateway/framework-integrations/pydantic-ai)


- [App Attribution](https://vercel.com/docs/ai-gateway/app-attribution)

- [Image Generation](https://vercel.com/docs/ai-gateway/image-generation)

- [Model Variants](https://vercel.com/docs/ai-gateway/model-variants)


- [MCP](https://vercel.com/docs/mcp)

Expand menu



- [Deploy MCP servers](https://vercel.com/docs/mcp/deploy-mcp-servers-to-vercel)

- [Vercel MCP server\\
beta](https://vercel.com/docs/mcp/vercel-mcp)

Expand menu



- [Tools](https://vercel.com/docs/mcp/vercel-mcp/tools)


- [Integrations for Agents](https://vercel.com/docs/agent-integrations)

- [Integrations for Models](https://vercel.com/docs/ai)

Expand menu



- [Adding a Provider](https://vercel.com/docs/ai/adding-a-provider)

- [Adding a Model](https://vercel.com/docs/ai/adding-a-model)

- [xAI](https://vercel.com/docs/ai/xai)

- [Groq](https://vercel.com/docs/ai/groq)

- [fal](https://vercel.com/docs/ai/fal)

- [Deep Infra](https://vercel.com/docs/ai/deepinfra)

- [ElevenLabs](https://vercel.com/docs/ai/elevenlabs)

- [LMNT](https://vercel.com/docs/ai/lmnt)

- [OpenAI](https://vercel.com/docs/ai/openai)

- [Perplexity](https://vercel.com/docs/ai/perplexity)

- [Pinecone](https://vercel.com/docs/ai/pinecone)

- [Replicate](https://vercel.com/docs/ai/replicate)

- [Together AI](https://vercel.com/docs/ai/togetherai)


- [Workflow\\
beta](https://vercel.com/docs/workflow)


- API

Expand menu



- [Build Output API](https://vercel.com/docs/build-output-api)

Expand menu



- [Build Output Configuration](https://vercel.com/docs/build-output-api/configuration)

- [Features](https://vercel.com/docs/build-output-api/features)

- [Vercel Primitives](https://vercel.com/docs/build-output-api/primitives)


- Build & Deploy

Expand menu



- [Builds](https://vercel.com/docs/builds)

Expand menu



- [Build Features](https://vercel.com/docs/builds/build-features)

- [Build Image](https://vercel.com/docs/builds/build-image)

- [Build Queues](https://vercel.com/docs/builds/build-queues)

- [Configuring a Build](https://vercel.com/docs/builds/configure-a-build)

- [Managing Builds](https://vercel.com/docs/builds/managing-builds)


- [Deploy Hooks](https://vercel.com/docs/deploy-hooks)

- [Deployment Checks](https://vercel.com/docs/deployment-checks)

- [Deployment Retention](https://vercel.com/docs/deployment-retention)

- [Deployments](https://vercel.com/docs/deployments)

Expand menu



- [Environments](https://vercel.com/docs/deployments/environments)

- [Generated URLs](https://vercel.com/docs/deployments/generated-urls)

- [Managing Deployments](https://vercel.com/docs/deployments/managing-deployments)

- [Promoting Deployments](https://vercel.com/docs/deployments/promoting-a-deployment)

- [Troubleshoot Build Errors](https://vercel.com/docs/deployments/troubleshoot-a-build)

- [Accessing Build Logs](https://vercel.com/docs/deployments/logs)

- [Claim Deployments](https://vercel.com/docs/deployments/claim-deployments)

- [Inspect OG Metadata](https://vercel.com/docs/deployments/og-preview)

- [Preview Deployment Suffix](https://vercel.com/docs/deployments/preview-deployment-suffix)

- [Sharing a Preview Deployment](https://vercel.com/docs/deployments/sharing-deployments)

- [Troubleshoot project collaboration](https://vercel.com/docs/deployments/troubleshoot-project-collaboration)


- [Environment Variables](https://vercel.com/docs/environment-variables)

Expand menu



- [Framework Environment Variables](https://vercel.com/docs/environment-variables/framework-environment-variables)

- [Managing Environment Variables](https://vercel.com/docs/environment-variables/managing-environment-variables)

- [Reserved Environment Variables](https://vercel.com/docs/environment-variables/reserved-environment-variables)

- [Sensitive Environment Variables](https://vercel.com/docs/environment-variables/sensitive-environment-variables)

- [Shared Environment Variables](https://vercel.com/docs/environment-variables/shared-environment-variables)

- [System Environment Variables](https://vercel.com/docs/environment-variables/system-environment-variables)


- [Git Integrations](https://vercel.com/docs/git)

Expand menu



- [GitHub](https://vercel.com/docs/git/vercel-for-github)

- [Azure DevOps](https://vercel.com/docs/git/vercel-for-azure-pipelines)

- [Bitbucket](https://vercel.com/docs/git/vercel-for-bitbucket)

- [GitLab](https://vercel.com/docs/git/vercel-for-gitlab)


- [Instant Rollback](https://vercel.com/docs/instant-rollback)

- [Microfrontends](https://vercel.com/docs/microfrontends)

Expand menu



- [Getting Started](https://vercel.com/docs/microfrontends/quickstart)

- [Local Development](https://vercel.com/docs/microfrontends/local-development)

- [Path Routing](https://vercel.com/docs/microfrontends/path-routing)

- [Configuration](https://vercel.com/docs/microfrontends/configuration)

- [Managing Microfrontends](https://vercel.com/docs/microfrontends/managing-microfrontends)

Expand menu



- [Security](https://vercel.com/docs/microfrontends/managing-microfrontends/security)

- [Using Vercel Toolbar](https://vercel.com/docs/microfrontends/managing-microfrontends/vercel-toolbar)


- [Testing & Troubleshooting](https://vercel.com/docs/microfrontends/troubleshooting)


- [Monorepos](https://vercel.com/docs/monorepos)

Expand menu



- [Turborepo](https://vercel.com/docs/monorepos/turborepo)

- [Remote Caching](https://vercel.com/docs/monorepos/remote-caching)

- [Nx](https://vercel.com/docs/monorepos/nx)

- [Monorepos FAQ](https://vercel.com/docs/monorepos/monorepo-faq)


- [Package Managers](https://vercel.com/docs/package-managers)

- [Protected Git Scopes](https://vercel.com/docs/protected-git-scopes)

- [Rolling Releases](https://vercel.com/docs/rolling-releases)

- [Skew Protection](https://vercel.com/docs/skew-protection)

- [Webhooks](https://vercel.com/docs/webhooks)

Expand menu



- [Webhooks API Reference](https://vercel.com/docs/webhooks/webhooks-api)


- CDN

Expand menu



- [Overview](https://vercel.com/docs/cdn)

- [Regions](https://vercel.com/docs/regions)

- [Headers](https://vercel.com/docs/headers)

Expand menu



- [Security Headers](https://vercel.com/docs/headers/security-headers)

- [Cache-Control Headers](https://vercel.com/docs/headers/cache-control-headers)

- [Request Headers](https://vercel.com/docs/headers/request-headers)

- [Response Headers](https://vercel.com/docs/headers/response-headers)


- [Cache](https://vercel.com/docs/edge-cache)

Expand menu



- [Purge](https://vercel.com/docs/edge-cache/purge)


- [Encryption](https://vercel.com/docs/encryption)

- [Compression](https://vercel.com/docs/compression)

- [Incremental Static Regeneration](https://vercel.com/docs/incremental-static-regeneration)

Expand menu



- [Getting Started](https://vercel.com/docs/incremental-static-regeneration/quickstart)

- [Usage & Pricing](https://vercel.com/docs/incremental-static-regeneration/limits-and-pricing)


- [Redirects](https://vercel.com/docs/redirects)

Expand menu



- [Configuration Redirects](https://vercel.com/docs/redirects/configuration-redirects)

- [Bulk redirects](https://vercel.com/docs/redirects/bulk-redirects)

Expand menu



- [Getting Started](https://vercel.com/docs/redirects/bulk-redirects/getting-started)


- [Rewrites](https://vercel.com/docs/rewrites)

- [Domains](https://vercel.com/docs/domains)

Expand menu



- [Working with Domains](https://vercel.com/docs/domains/working-with-domains)

Expand menu



- [Adding a Domain](https://vercel.com/docs/domains/working-with-domains/add-a-domain)

- [Adding a Domain to an Environment](https://vercel.com/docs/domains/working-with-domains/add-a-domain-to-environment)

- [Assigning a Domain to a Git Branch](https://vercel.com/docs/domains/working-with-domains/assign-domain-to-a-git-branch)

- [Deploying & Redirecting Domains](https://vercel.com/docs/domains/working-with-domains/deploying-and-redirecting)

- [Removing a Domain](https://vercel.com/docs/domains/working-with-domains/remove-a-domain)

- [Renewing a Domain](https://vercel.com/docs/domains/working-with-domains/renew-a-domain)

- [Transferring Domains](https://vercel.com/docs/domains/working-with-domains/transfer-your-domain)

- [Viewing & Searching Domains](https://vercel.com/docs/domains/working-with-domains/view-and-search-domains)


- [Working with DNS](https://vercel.com/docs/domains/working-with-dns)

- [Managing DNS Records](https://vercel.com/docs/domains/managing-dns-records)

- [Working with Nameservers](https://vercel.com/docs/domains/working-with-nameservers)

- [Managing Nameservers](https://vercel.com/docs/domains/managing-nameservers)

- [Working with SSL](https://vercel.com/docs/domains/working-with-ssl)

- [Custom SSL Certificates](https://vercel.com/docs/domains/custom-SSL-certificate)

- [Pre-Generate SSL Certificates](https://vercel.com/docs/domains/pre-generating-ssl-certs)

- [Supported Domains](https://vercel.com/docs/domains/supported-domains)

- [Troubleshooting Domains](https://vercel.com/docs/domains/troubleshooting)

- [Using Domains API](https://vercel.com/docs/domains/registrar-api)


- [Image Optimization](https://vercel.com/docs/image-optimization)

Expand menu



- [Getting Started](https://vercel.com/docs/image-optimization/quickstart)

- [Limits and Pricing](https://vercel.com/docs/image-optimization/limits-and-pricing)

- [Managing Usage & Costs](https://vercel.com/docs/image-optimization/managing-image-optimization-costs)

- [Legacy Pricing](https://vercel.com/docs/image-optimization/legacy-pricing)


- [Manage CDN Usage](https://vercel.com/docs/manage-cdn-usage)

- [Request Collapsing](https://vercel.com/docs/request-collapsing)


- [CLI](https://vercel.com/docs/cli)

Expand menu



- [Deploying from CLI](https://vercel.com/docs/cli/deploying-from-cli)

- [Project Linking](https://vercel.com/docs/cli/project-linking)

- [Telemetry](https://vercel.com/docs/cli/about-telemetry)

- [Global Options](https://vercel.com/docs/cli/global-options)

- [`vercel alias`](https://vercel.com/docs/cli/alias)

- [`vercel bisect`](https://vercel.com/docs/cli/bisect)

- [`vercel blob`](https://vercel.com/docs/cli/blob)

- [`vercel build`](https://vercel.com/docs/cli/build)

- [`vercel cache`](https://vercel.com/docs/cli/cache)

- [`vercel certs`](https://vercel.com/docs/cli/certs)

- [`vercel curl`](https://vercel.com/docs/cli/curl)

- [`vercel deploy`](https://vercel.com/docs/cli/deploy)

- [`vercel dev`](https://vercel.com/docs/cli/dev)

- [`vercel dns`](https://vercel.com/docs/cli/dns)

- [`vercel domains`](https://vercel.com/docs/cli/domains)

- [`vercel env`](https://vercel.com/docs/cli/env)

- [`vercel git`](https://vercel.com/docs/cli/git)

- [`vercel help`](https://vercel.com/docs/cli/help)

- [`vercel httpstat`](https://vercel.com/docs/cli/httpstat)

- [`vercel init`](https://vercel.com/docs/cli/init)

- [`vercel inspect`](https://vercel.com/docs/cli/inspect)

- [`vercel install`](https://vercel.com/docs/cli/install)

- [`vercel integration`](https://vercel.com/docs/cli/integration)

- [`vercel integration-resource`](https://vercel.com/docs/cli/integration-resource)

- [`vercel link`](https://vercel.com/docs/cli/link)

- [`vercel list`](https://vercel.com/docs/cli/list)

- [`vercel login`](https://vercel.com/docs/cli/login)

- [`vercel logout`](https://vercel.com/docs/cli/logout)

- [`vercel logs`](https://vercel.com/docs/cli/logs)

- [`vercel open`](https://vercel.com/docs/cli/open)

- [`vercel project`](https://vercel.com/docs/cli/project)

- [`vercel promote`](https://vercel.com/docs/cli/promote)

- [`vercel pull`](https://vercel.com/docs/cli/pull)

- [`vercel redeploy`](https://vercel.com/docs/cli/redeploy)

- [`vercel remove`](https://vercel.com/docs/cli/remove)

- [`vercel rollback`](https://vercel.com/docs/cli/rollback)

- [`vercel rolling-release`](https://vercel.com/docs/cli/rolling-release)

- [`vercel switch`](https://vercel.com/docs/cli/switch)

- [`vercel teams`](https://vercel.com/docs/cli/teams)

- [`vercel telemetry`](https://vercel.com/docs/cli/telemetry)

- [`vercel whoami`](https://vercel.com/docs/cli/whoami)


- Collaboration

Expand menu



- [Comments](https://vercel.com/docs/comments)

Expand menu



- [Enabling Comments](https://vercel.com/docs/comments/how-comments-work)

- [Using Comments](https://vercel.com/docs/comments/using-comments)

- [Managing Comments](https://vercel.com/docs/comments/managing-comments)

- [Integrations](https://vercel.com/docs/comments/integrations)


- [Draft Mode](https://vercel.com/docs/draft-mode)

- [Edit Mode](https://vercel.com/docs/edit-mode)

- [Feature Flags](https://vercel.com/docs/feature-flags)

Expand menu



- [Flags Explorer](https://vercel.com/docs/feature-flags/flags-explorer)

Expand menu



- [Getting Started](https://vercel.com/docs/feature-flags/flags-explorer/getting-started)

- [Reference](https://vercel.com/docs/feature-flags/flags-explorer/reference)

- [Pricing](https://vercel.com/docs/feature-flags/flags-explorer/limits-and-pricing)


- [Flags SDK](https://vercel.com/docs/feature-flags/feature-flags-pattern)

- [With Runtime Logs](https://vercel.com/docs/feature-flags/integrate-with-runtime-logs)

- [With Vercel Platform](https://vercel.com/docs/feature-flags/integrate-vercel-platform)

- [With Web Analytics](https://vercel.com/docs/feature-flags/integrate-with-web-analytics)


- [Toolbar](https://vercel.com/docs/vercel-toolbar)

Expand menu



- [Add to Environments](https://vercel.com/docs/vercel-toolbar/in-production-and-localhost)

Expand menu



- [Add to Localhost](https://vercel.com/docs/vercel-toolbar/in-production-and-localhost/add-to-localhost)

- [Add to Production](https://vercel.com/docs/vercel-toolbar/in-production-and-localhost/add-to-production)


- [Managing Toolbar](https://vercel.com/docs/vercel-toolbar/managing-toolbar)

- [Browser Extensions](https://vercel.com/docs/vercel-toolbar/browser-extension)

- [Accessibility Audit Tool](https://vercel.com/docs/vercel-toolbar/accessibility-audit-tool)

- [Interaction Timing Tool](https://vercel.com/docs/vercel-toolbar/interaction-timing-tool)

- [Layout Shift Tool](https://vercel.com/docs/vercel-toolbar/layout-shift-tool)


- Compute

Expand menu



- [Fluid Compute](https://vercel.com/docs/fluid-compute)

- [Functions](https://vercel.com/docs/functions)

Expand menu



- [Getting Started](https://vercel.com/docs/functions/quickstart)

- [Streaming](https://vercel.com/docs/functions/streaming-functions)

- [Runtimes](https://vercel.com/docs/functions/runtimes)

Expand menu



- [Node.js](https://vercel.com/docs/functions/runtimes/node-js)

Expand menu



- [Advanced Node.js Usage](https://vercel.com/docs/functions/runtimes/node-js/advanced-node-configuration)

- [Supported Node.js versions](https://vercel.com/docs/functions/runtimes/node-js/node-js-versions)


- [Bun](https://vercel.com/docs/functions/runtimes/bun)

- [Python](https://vercel.com/docs/functions/runtimes/python)

- [Go RuntimeGo](https://vercel.com/docs/functions/runtimes/go)

- [Ruby](https://vercel.com/docs/functions/runtimes/ruby)

- [Wasm](https://vercel.com/docs/functions/runtimes/wasm)

- [Edge Runtime](https://vercel.com/docs/functions/runtimes/edge)


- [Configuring Functions](https://vercel.com/docs/functions/configuring-functions)

Expand menu



- [Duration](https://vercel.com/docs/functions/configuring-functions/duration)

- [Memory](https://vercel.com/docs/functions/configuring-functions/memory)

- [Runtime](https://vercel.com/docs/functions/configuring-functions/runtime)

- [Region](https://vercel.com/docs/functions/configuring-functions/region)

- [Advanced Configuration](https://vercel.com/docs/functions/configuring-functions/advanced-configuration)


- [API Reference](https://vercel.com/docs/functions/functions-api-reference)

Expand menu



- [Node.js](https://vercel.com/docs/functions/functions-api-reference/vercel-functions-package)

- [Python](https://vercel.com/docs/functions/functions-api-reference/vercel-sdk-python)


- [Logs](https://vercel.com/docs/functions/logs)

- [Limits](https://vercel.com/docs/functions/limitations)

- [Concurrency Scaling](https://vercel.com/docs/functions/concurrency-scaling)

- [Pricing](https://vercel.com/docs/functions/usage-and-pricing)

Expand menu



- [Legacy Usage & Pricing](https://vercel.com/docs/functions/usage-and-pricing/legacy-pricing)


- [Data Cache](https://vercel.com/docs/data-cache)

- [Routing Middleware](https://vercel.com/docs/routing-middleware)

Expand menu



- [Getting Started](https://vercel.com/docs/routing-middleware/getting-started)

- [API](https://vercel.com/docs/routing-middleware/api)


- [Cron Jobs](https://vercel.com/docs/cron-jobs)

Expand menu



- [Getting Started](https://vercel.com/docs/cron-jobs/quickstart)

- [Managing Cron Jobs](https://vercel.com/docs/cron-jobs/manage-cron-jobs)

- [Usage & Pricing](https://vercel.com/docs/cron-jobs/usage-and-pricing)


- [OG Image Generation](https://vercel.com/docs/og-image-generation)

Expand menu



- [`@vercel/og`](https://vercel.com/docs/og-image-generation/og-image-api)

- [Examples](https://vercel.com/docs/og-image-generation/examples)


- [Sandbox\\
beta](https://vercel.com/docs/vercel-sandbox)

Expand menu



- [CLI Reference](https://vercel.com/docs/vercel-sandbox/cli-reference)

- [Examples](https://vercel.com/docs/vercel-sandbox/examples)

- [Pricing and Limits](https://vercel.com/docs/vercel-sandbox/pricing)


- [Multi-tenant](https://vercel.com/docs/multi-tenant)

Expand menu



- [Domain Management](https://vercel.com/docs/multi-tenant/domain-management)

- [Limits](https://vercel.com/docs/multi-tenant/limits)


- Observability

Expand menu



- [Overview](https://vercel.com/docs/observability)

Expand menu



- [Insights](https://vercel.com/docs/observability/insights)

- [Observability Plus](https://vercel.com/docs/observability/observability-plus)


- [Alerts\\
beta](https://vercel.com/docs/alerts)

- [Logs](https://vercel.com/docs/logs)

Expand menu



- [Runtime](https://vercel.com/docs/logs/runtime)


- [Tracing](https://vercel.com/docs/tracing)

Expand menu



- [Instrumentation](https://vercel.com/docs/tracing/instrumentation)

- [Session Tracing](https://vercel.com/docs/tracing/session-tracing)


- [Query](https://vercel.com/docs/query)

Expand menu



- [Query Reference](https://vercel.com/docs/query/reference)

- [Monitoring](https://vercel.com/docs/query/monitoring)

Expand menu



- [Getting Started](https://vercel.com/docs/query/monitoring/quickstart)

- [Monitoring Reference](https://vercel.com/docs/query/monitoring/monitoring-reference)

- [Limits and Pricing](https://vercel.com/docs/query/monitoring/limits-and-pricing)


- [Notebooks](https://vercel.com/docs/notebooks)

- [Speed Insights](https://vercel.com/docs/speed-insights)

Expand menu



- [Getting Started](https://vercel.com/docs/speed-insights/quickstart)

- [Using Speed Insights](https://vercel.com/docs/speed-insights/using-speed-insights)

- [Metrics](https://vercel.com/docs/speed-insights/metrics)

- [Privacy](https://vercel.com/docs/speed-insights/privacy-policy)

- [`@vercel/speed-insights`](https://vercel.com/docs/speed-insights/package)

- [Limits and Pricing](https://vercel.com/docs/speed-insights/limits-and-pricing)

- [Troubleshooting](https://vercel.com/docs/speed-insights/troubleshooting)

- [Migrating from Legacy](https://vercel.com/docs/speed-insights/migrating-from-legacy)


- [Drains](https://vercel.com/docs/drains)

Expand menu



- [Using Drains](https://vercel.com/docs/drains/using-drains)

- Reference

Expand menu



- [Logs](https://vercel.com/docs/drains/reference/logs)

- [Traces](https://vercel.com/docs/drains/reference/traces)

- [Speed Insights](https://vercel.com/docs/drains/reference/speed-insights)

- [Web Analytics](https://vercel.com/docs/drains/reference/analytics)


- [Security](https://vercel.com/docs/drains/security)


- [Web Analytics](https://vercel.com/docs/analytics)

Expand menu



- [Getting Started](https://vercel.com/docs/analytics/quickstart)

- [Using Web Analytics](https://vercel.com/docs/analytics/using-web-analytics)

- [Filtering](https://vercel.com/docs/analytics/filtering)

- [Custom Events](https://vercel.com/docs/analytics/custom-events)

- [Redacting Sensitive Data](https://vercel.com/docs/analytics/redacting-sensitive-data)

- [Privacy](https://vercel.com/docs/analytics/privacy-policy)

- [`@vercel/analytics`](https://vercel.com/docs/analytics/package)

- [Pricing](https://vercel.com/docs/analytics/limits-and-pricing)

- [Troubleshooting](https://vercel.com/docs/analytics/troubleshooting)


- [Manage & Optimize](https://vercel.com/docs/manage-and-optimize-observability)


- Platform

Expand menu



- [Dashboard](https://vercel.com/docs/dashboard-features)

Expand menu



- [Navigating the Dashboard](https://vercel.com/docs/dashboard-features/overview)

- [Support Center](https://vercel.com/docs/dashboard-features/support-center)

- [Using the Command Menu](https://vercel.com/docs/dashboard-features/command-menu)


- [Notifications](https://vercel.com/docs/notifications)

- [Projects](https://vercel.com/docs/projects)

Expand menu



- [Managing projects](https://vercel.com/docs/projects/managing-projects)

- [Project Dashboard](https://vercel.com/docs/projects/project-dashboard)

- [Transferring a project](https://vercel.com/docs/projects/transferring-projects)


- [Project Configuration](https://vercel.com/docs/project-configuration)

Expand menu



- [General Settings](https://vercel.com/docs/project-configuration/general-settings)

- [Project Settings](https://vercel.com/docs/project-configuration/project-settings)

- [Git Configuration](https://vercel.com/docs/project-configuration/git-configuration)

- [Git Settings](https://vercel.com/docs/project-configuration/git-settings)

- [Global Configuration](https://vercel.com/docs/project-configuration/global-configuration)

- [Security settings](https://vercel.com/docs/project-configuration/security-settings)


- [Checks](https://vercel.com/docs/checks)

Expand menu



- [Checks API](https://vercel.com/docs/checks/checks-api)

- [Checks Reference](https://vercel.com/docs/checks/creating-checks)


- [Glossary](https://vercel.com/docs/glossary)

- [Integrations](https://vercel.com/docs/integrations)

Expand menu



- [Extend Vercel](https://vercel.com/docs/integrations/install-an-integration)

Expand menu



- [Add a Connectable Account](https://vercel.com/docs/integrations/install-an-integration/add-a-connectable-account)

- [Add a Native Integration](https://vercel.com/docs/integrations/install-an-integration/product-integration)

- [Permissions and Access](https://vercel.com/docs/integrations/install-an-integration/manage-integrations-reference)


- [Integrate with Vercel](https://vercel.com/docs/integrations/create-integration)

Expand menu



- [Native integration concepts](https://vercel.com/docs/integrations/create-integration/native-integration)

- [Create a Native Integration](https://vercel.com/docs/integrations/create-integration/marketplace-product)

- [Deployment integration actions](https://vercel.com/docs/integrations/create-integration/deployment-integration-action)

- [Native Integration Flows](https://vercel.com/docs/integrations/create-integration/marketplace-flows)

- [Native Integrations REST API](https://vercel.com/docs/integrations/create-integration/marketplace-api)

- [Integration Approval Checklist](https://vercel.com/docs/integrations/create-integration/approval-checklist)

- [Integration Image Guidelines](https://vercel.com/docs/integrations/create-integration/integration-image-guidelines)

- [Requirements for listing an Integration](https://vercel.com/docs/integrations/create-integration/submit-integration)

- [Upgrade an Integration](https://vercel.com/docs/integrations/create-integration/upgrade-integration)


- [CMS Integrations](https://vercel.com/docs/integrations/cms)

Expand menu



- [Agility CMS](https://vercel.com/docs/integrations/cms/agility-cms)

- [ButterCMS](https://vercel.com/docs/integrations/cms/butter-cms)

- [Contentful](https://vercel.com/docs/integrations/cms/contentful)

- [DatoCMS](https://vercel.com/docs/integrations/cms/dato-cms)

- [Formspree](https://vercel.com/docs/integrations/cms/formspree)

- [Makeswift](https://vercel.com/docs/integrations/cms/makeswift)

- [Sanity](https://vercel.com/docs/integrations/cms/sanity)

- [Sitecore](https://vercel.com/docs/integrations/cms/sitecore)


- [Ecommerce Integrations](https://vercel.com/docs/integrations/ecommerce)

Expand menu



- [BigCommerce](https://vercel.com/docs/integrations/ecommerce/bigcommerce)

- [Shopify](https://vercel.com/docs/integrations/ecommerce/shopify)


- [Sign in with Vercel](https://vercel.com/docs/integrations/sign-in-with-vercel)

- [Building Integrations with Vercel REST API](https://vercel.com/docs/integrations/vercel-api-integrations)

- External Platforms

Expand menu



- [Kubernetes](https://vercel.com/docs/integrations/external-platforms/kubernetes)


- [Limits](https://vercel.com/docs/limits)

Expand menu



- [Fair use Guidelines](https://vercel.com/docs/limits/fair-use-guidelines)


- Pricing

Expand menu



- [Plans](https://vercel.com/docs/plans)

Expand menu



- [Hobby Plan](https://vercel.com/docs/plans/hobby)

- [Pro Plan](https://vercel.com/docs/plans/pro-plan)

Expand menu



- [Pro Plan Trial](https://vercel.com/docs/plans/pro-plan/trials)

- [Switch to the Pro Plan](https://vercel.com/docs/plans/pro-plan/switching)

- [Billing FAQ](https://vercel.com/docs/plans/pro-plan/billing)


- [Enterprise Plan](https://vercel.com/docs/plans/enterprise)

Expand menu



- [Billing FAQ](https://vercel.com/docs/plans/enterprise/billing)

- [MIUs for AI](https://vercel.com/docs/plans/enterprise/buy-with-miu)


- [Legacy Pro Plan](https://vercel.com/docs/plans/pro)

Expand menu



- [Billing FAQ](https://vercel.com/docs/plans/pro/billing)


- [Pricing](https://vercel.com/docs/pricing)

Expand menu



- [Regional Pricing](https://vercel.com/docs/pricing/regional-pricing)

Expand menu



- [Cape Town, South Africa](https://vercel.com/docs/pricing/regional-pricing/cpt1)

- [Cleveland, USA](https://vercel.com/docs/pricing/regional-pricing/cle1)

- [Dubai, UAE](https://vercel.com/docs/pricing/regional-pricing/dxb1)

- [Dublin, Ireland](https://vercel.com/docs/pricing/regional-pricing/dub1)

- [Frankfurt, Germany](https://vercel.com/docs/pricing/regional-pricing/fra1)

- [Hong Kong](https://vercel.com/docs/pricing/regional-pricing/hkg1)

- [London, UK](https://vercel.com/docs/pricing/regional-pricing/lhr1)

- [Mumbai, India](https://vercel.com/docs/pricing/regional-pricing/bom1)

- [Osaka, Japan](https://vercel.com/docs/pricing/regional-pricing/kix1)

- [Paris, France](https://vercel.com/docs/pricing/regional-pricing/cdg1)

- [Portland, USA](https://vercel.com/docs/pricing/regional-pricing/pdx1)

- [San Francisco, USA](https://vercel.com/docs/pricing/regional-pricing/sfo1)

- [São Paulo, Brazil](https://vercel.com/docs/pricing/regional-pricing/gru1)

- [Seoul, South Korea](https://vercel.com/docs/pricing/regional-pricing/icn1)

- [Singapore](https://vercel.com/docs/pricing/regional-pricing/sin1)

- [Stockholm, Sweden](https://vercel.com/docs/pricing/regional-pricing/arn1)

- [Sydney, Australia](https://vercel.com/docs/pricing/regional-pricing/syd1)

- [Tokyo, Japan](https://vercel.com/docs/pricing/regional-pricing/hnd1)

- [Washington, D.C., USA](https://vercel.com/docs/pricing/regional-pricing/iad1)


- [Manage and Optimize Usage](https://vercel.com/docs/pricing/manage-and-optimize-usage)

- [Calculating Usage of Resources](https://vercel.com/docs/pricing/how-does-vercel-calculate-usage-of-resources)

- [Billing & Invoices](https://vercel.com/docs/pricing/understanding-my-invoice)

- [Legacy Metrics](https://vercel.com/docs/pricing/legacy)

- [Sales Tax](https://vercel.com/docs/pricing/sales-tax)


- [Spend Management](https://vercel.com/docs/spend-management)


- Security

Expand menu



- [Overview](https://vercel.com/docs/security)

Expand menu



- [Security & Compliance Measures](https://vercel.com/docs/security/compliance)

- [Shared Responsibility Model](https://vercel.com/docs/security/shared-responsibility)

- [PCI DSS iframe Integration](https://vercel.com/docs/security/pci-dss)

- [Reverse Proxy Servers and Vercel](https://vercel.com/docs/security/reverse-proxy)

- [Access Control](https://vercel.com/docs/security/access-control)


- [Audit Logs](https://vercel.com/docs/audit-log)

- [Firewall](https://vercel.com/docs/vercel-firewall)

Expand menu



- [Firewall Concepts](https://vercel.com/docs/vercel-firewall/firewall-concepts)

- [DDoS Mitigation](https://vercel.com/docs/vercel-firewall/ddos-mitigation)

- [Attack Challenge Mode](https://vercel.com/docs/vercel-firewall/attack-challenge-mode)

- [Web Application Firewall](https://vercel.com/docs/vercel-firewall/vercel-waf)

Expand menu



- [Custom Rules](https://vercel.com/docs/vercel-firewall/vercel-waf/custom-rules)

- [Rate Limiting](https://vercel.com/docs/vercel-firewall/vercel-waf/rate-limiting)

- [Rule Configuration](https://vercel.com/docs/vercel-firewall/vercel-waf/rule-configuration)

- [System Bypass Rules](https://vercel.com/docs/vercel-firewall/vercel-waf/system-bypass-rules)

- [Rate Limiting SDK](https://vercel.com/docs/vercel-firewall/vercel-waf/rate-limiting-sdk)

- [IP Blocking](https://vercel.com/docs/vercel-firewall/vercel-waf/ip-blocking)

- [Managed Rulesets](https://vercel.com/docs/vercel-firewall/vercel-waf/managed-rulesets)

- [Examples](https://vercel.com/docs/vercel-firewall/vercel-waf/examples)

- [Usage & Pricing](https://vercel.com/docs/vercel-firewall/vercel-waf/usage-and-pricing)


- [Firewall API](https://vercel.com/docs/vercel-firewall/firewall-api)

- [Firewall Observability](https://vercel.com/docs/vercel-firewall/firewall-observability)


- [Bot Management](https://vercel.com/docs/bot-management)

- [BotID](https://vercel.com/docs/botid)

Expand menu



- [Get Started with BotID](https://vercel.com/docs/botid/get-started)

- [Handling Verified Bots](https://vercel.com/docs/botid/verified-bots)

- [Advanced BotID Configuration](https://vercel.com/docs/botid/advanced-configuration)

- [Form Submissions](https://vercel.com/docs/botid/form-submissions)

- [Local Development Behavior](https://vercel.com/docs/botid/local-development-behavior)


- [Connectivity](https://vercel.com/docs/connectivity)

Expand menu



- [Secure Compute](https://vercel.com/docs/connectivity/secure-compute)

- [Static IPs](https://vercel.com/docs/connectivity/static-ips)

Expand menu



- [Getting Started](https://vercel.com/docs/connectivity/static-ips/getting-started)


- [OIDC](https://vercel.com/docs/oidc)

Expand menu



- [AWS](https://vercel.com/docs/oidc/aws)

- [Azure](https://vercel.com/docs/oidc/azure)

- [Connect your API](https://vercel.com/docs/oidc/api)

- [Google Cloud Platform](https://vercel.com/docs/oidc/gcp)

- [OIDC Reference](https://vercel.com/docs/oidc/reference)


- [RBAC](https://vercel.com/docs/rbac)

Expand menu



- [Access Roles](https://vercel.com/docs/rbac/access-roles)

Expand menu



- [Extended Permissions](https://vercel.com/docs/rbac/access-roles/extended-permissions)

- [Project Level Roles](https://vercel.com/docs/rbac/access-roles/project-level-roles)

- [Team Level Roles](https://vercel.com/docs/rbac/access-roles/team-level-roles)


- [Access Groups](https://vercel.com/docs/rbac/access-groups)

- [Managing Team Members](https://vercel.com/docs/rbac/managing-team-members)


- [Two-factor Enforcement](https://vercel.com/docs/two-factor-enforcement)


- Storage

Expand menu



- [Blob](https://vercel.com/docs/vercel-blob)

Expand menu



- [Server Uploads](https://vercel.com/docs/vercel-blob/server-upload)

- [Client Uploads](https://vercel.com/docs/vercel-blob/client-upload)

- [Using the SDK](https://vercel.com/docs/vercel-blob/using-blob-sdk)

- [Pricing](https://vercel.com/docs/vercel-blob/usage-and-pricing)

- [Security](https://vercel.com/docs/vercel-blob/security)

- [Examples](https://vercel.com/docs/vercel-blob/examples)


- [Edge Config](https://vercel.com/docs/edge-config)

Expand menu



- [Getting Started](https://vercel.com/docs/edge-config/get-started)

- [Using Edge Config](https://vercel.com/docs/edge-config/using-edge-config)

- [Edge Configs & REST API](https://vercel.com/docs/edge-config/vercel-api)

- [Edge Configs & Dashboard](https://vercel.com/docs/edge-config/edge-config-dashboard)

- [Edge Config SDK](https://vercel.com/docs/edge-config/edge-config-sdk)

- [Limits & Pricing](https://vercel.com/docs/edge-config/edge-config-limits)

- [Integrations](https://vercel.com/docs/edge-config/edge-config-integrations)

Expand menu



- [DevCycle](https://vercel.com/docs/edge-config/edge-config-integrations/devcycle-edge-config)

- [Hypertune](https://vercel.com/docs/edge-config/edge-config-integrations/hypertune-edge-config)

- [LaunchDarkly](https://vercel.com/docs/edge-config/edge-config-integrations/launchdarkly-edge-config)

- [Split](https://vercel.com/docs/edge-config/edge-config-integrations/split-edge-config)

- [Statsig](https://vercel.com/docs/edge-config/edge-config-integrations/statsig-edge-config)


[AI Gateway](https://vercel.com/docs/ai-gateway)

OpenAI-Compatible API

# OpenAI-Compatible API

Copy page

Ask AI about this page

Last updatedOctober 23, 2025

AI Gateway provides OpenAI-compatible API endpoints, letting you use multiple AI providers through a familiar interface. You can use existing OpenAI client libraries, switch to the AI Gateway with a URL change, and keep your current tools and workflows without code rewrites.

The OpenAI-compatible API implements the same specification as the [OpenAI API](https://platform.openai.com/docs/api-reference/chat).

## [Base URL](https://vercel.com/docs/ai-gateway/openai-compat\#base-url)

The OpenAI-compatible API is available at the following base URL:

`https://ai-gateway.vercel.sh/v1
`

## [Authentication](https://vercel.com/docs/ai-gateway/openai-compat\#authentication)

The OpenAI-compatible API supports the same authentication methods as the main AI Gateway:

- API key: Use your AI Gateway API key with the `Authorization: Bearer <token>` header
- OIDC token: Use your Vercel OIDC token with the `Authorization: Bearer <token>` header

You only need to use one of these forms of authentication. If an API key is specified it will take precedence over any OIDC token, even if the API key is invalid.

## [Supported endpoints](https://vercel.com/docs/ai-gateway/openai-compat\#supported-endpoints)

The AI Gateway supports the following OpenAI-compatible endpoints:

- [`GET /models`](https://vercel.com/docs/ai-gateway/openai-compat#list-models) \- List available models
- [`GET /models/{model}`](https://vercel.com/docs/ai-gateway/openai-compat#retrieve-model) \- Retrieve a specific model
- [`POST /chat/completions`](https://vercel.com/docs/ai-gateway/openai-compat#chat-completions) \- Create chat completions with support for streaming, attachments, tool calls, and image generation
- [`POST /embeddings`](https://vercel.com/docs/ai-gateway/openai-compat#embeddings) \- Generate vector embeddings

## [Integration with existing tools](https://vercel.com/docs/ai-gateway/openai-compat\#integration-with-existing-tools)

You can use the AI Gateway's OpenAI-compatible API with existing tools and
libraries like the [OpenAI client libraries](https://platform.openai.com/docs/libraries) and [AI SDK 4](https://v4.ai-sdk.dev/). Point your existing
client to the AI Gateway's base URL and use your AI Gateway [API key](https://vercel.com/docs/ai-gateway/authentication#api-key) or [OIDC token](https://vercel.com/docs/ai-gateway/authentication#oidc-token) for authentication.

### [OpenAI client libraries](https://vercel.com/docs/ai-gateway/openai-compat\#openai-client-libraries)

TypeScriptPython

client.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const response = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [{ role: 'user', content: 'Hello, world!' }],
});
```

client.py

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

response = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {'role': 'user', 'content': 'Hello, world!'}\
    ]
)
```

### [AI SDK 4](https://vercel.com/docs/ai-gateway/openai-compat\#ai-sdk-4)

For compatibility with [AI SDK v4](https://v4.ai-sdk.dev/) and AI Gateway, install the [@ai-sdk/openai-compatible](https://ai-sdk.dev/providers/openai-compatible-providers) package.

Verify that you are using AI SDK 4 by using the following package versions: `@ai-sdk/openai-compatible` version
`<1.0.0` (e.g., `0.2.16`) and `ai` version `<5.0.0` (e.g., `4.3.19`).

TypeScript

client.ts

```
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

const gateway = createOpenAICompatible({
  name: 'openai',
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const response = await generateText({
  model: gateway('anthropic/claude-sonnet-4'),
  prompt: 'Hello, world!',
});
```

## [List models](https://vercel.com/docs/ai-gateway/openai-compat\#list-models)

Retrieve a list of all available models that can be used with the AI Gateway.

Endpoint

`GET /models
`

Example request

TypeScriptPython

list-models.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const models = await openai.models.list();
console.log(models);
```

list-models.py

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

models = client.models.list()
print(models)
```

Response format

The response follows the OpenAI API format:

```
{
  "object": "list",
  "data": [\
    {\
      "id": "anthropic/claude-sonnet-4",\
      "object": "model",\
      "created": 1677610602,\
      "owned_by": "anthropic"\
    },\
    {\
      "id": "openai/gpt-4.1-mini",\
      "object": "model",\
      "created": 1677610602,\
      "owned_by": "openai"\
    }\
  ]
}
```

## [Retrieve model](https://vercel.com/docs/ai-gateway/openai-compat\#retrieve-model)

Retrieve details about a specific model.

Endpoint

`GET /models/{model}
`

Parameters

- `model` (required): The model ID to retrieve (e.g., `anthropic/claude-sonnet-4`)

Example request

TypeScriptPython

retrieve-model.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const model = await openai.models.retrieve('anthropic/claude-sonnet-4');
console.log(model);
```

retrieve-model.py

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

model = client.models.retrieve('anthropic/claude-sonnet-4')
print(model)
```

Response format

```
{
  "id": "anthropic/claude-sonnet-4",
  "object": "model",
  "created": 1677610602,
  "owned_by": "anthropic"
}
```

## [Chat completions](https://vercel.com/docs/ai-gateway/openai-compat\#chat-completions)

Create chat completions using various AI models available through the AI Gateway.

Endpoint

`POST /chat/completions
`

### [Basic chat completion](https://vercel.com/docs/ai-gateway/openai-compat\#basic-chat-completion)

Create a non-streaming chat completion.

Example request

TypeScriptPython

chat-completion.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: 'Write a one-sentence bedtime story about a unicorn.',\
    },\
  ],
  stream: false,
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Tokens used:', completion.usage);
```

chat-completion.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Write a one-sentence bedtime story about a unicorn.'\
        }\
    ],
    stream=False,
)

print('Assistant:', completion.choices[0].message.content)
print('Tokens used:', completion.usage)
```

Response format

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "anthropic/claude-sonnet-4",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "Once upon a time, a gentle unicorn with a shimmering silver mane danced through moonlit clouds, sprinkling stardust dreams upon sleeping children below."\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 28,
    "total_tokens": 43
  }
}
```

### [Streaming chat completion](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-chat-completion)

Create a streaming chat completion that streams tokens as they are generated.

Example request

TypeScriptPython

streaming-chat.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const stream = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: 'Write a one-sentence bedtime story about a unicorn.',\
    },\
  ],
  stream: true,
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
  }
}
```

streaming-chat.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

stream = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Write a one-sentence bedtime story about a unicorn.'\
        }\
    ],
    stream=True,
)

for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end='', flush=True)
```

#### [Streaming response format](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-response-format)

Streaming responses are sent as [Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events), a web standard for real-time data streaming over HTTP. Each event contains a JSON object with the partial response data.

The response format follows the OpenAI streaming specification:

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"anthropic/claude-sonnet-4","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"anthropic/claude-sonnet-4","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}

data: [DONE]
```

Key characteristics:

- Each line starts with `data:` followed by JSON
- Content is delivered incrementally in the `delta.content` field
- The stream ends with `data: [DONE]`
- Empty lines separate events

SSE Parsing Libraries:

If you're building custom SSE parsing (instead of using the OpenAI SDK), these libraries can help:

- JavaScript/TypeScript: [`eventsource-parser`](https://www.npmjs.com/package/eventsource-parser) \- Robust SSE parsing with support for partial events
- Python: [`httpx-sse`](https://pypi.org/project/httpx-sse/) \- SSE support for HTTPX, or [`sseclient-py`](https://pypi.org/project/sseclient-py/) for requests

For more details about the SSE specification, see the [W3C specification](https://html.spec.whatwg.org/multipage/server-sent-events.html).

### [Image attachments](https://vercel.com/docs/ai-gateway/openai-compat\#image-attachments)

Send images as part of your chat completion request.

Example request

TypeScriptPython

image-analysis.ts

```
import fs from 'node:fs';
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// Read the image file as base64
const imageBuffer = fs.readFileSync('./path/to/image.png');
const imageBase64 = imageBuffer.toString('base64');

const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: [\
        { type: 'text', text: 'Describe this image in detail.' },\
        {\
          type: 'image_url',\
          image_url: {\
            url: `data:image/png;base64,${imageBase64}`,\
            detail: 'auto',\
          },\
        },\
      ],\
    },\
  ],
  stream: false,
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Tokens used:', completion.usage);
```

image-analysis.py

```
import os
import base64
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

# Read the image file as base64
with open('./path/to/image.png', 'rb') as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': [\
                {'type': 'text', 'text': 'Describe this image in detail.'},\
                {\
                    'type': 'image_url',\
                    'image_url': {\
                        'url': f'data:image/png;base64,{image_base64}',\
                        'detail': 'auto'\
                    }\
                }\
            ]\
        }\
    ],
    stream=False,
)

print('Assistant:', completion.choices[0].message.content)
print('Tokens used:', completion.usage)
```

### [PDF attachments](https://vercel.com/docs/ai-gateway/openai-compat\#pdf-attachments)

Send PDF documents as part of your chat completion request.

Example request

TypeScriptPython

pdf-analysis.ts

```
import fs from 'node:fs';
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// Read the PDF file as base64
const pdfBuffer = fs.readFileSync('./path/to/document.pdf');
const pdfBase64 = pdfBuffer.toString('base64');

const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: [\
        {\
          type: 'text',\
          text: 'What is the main topic of this document? Please summarize the key points.',\
        },\
        {\
          type: 'file',\
          file: {\
            data: pdfBase64,\
            media_type: 'application/pdf',\
            filename: 'document.pdf',\
          },\
        },\
      ],\
    },\
  ],
  stream: false,
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Tokens used:', completion.usage);
```

pdf-analysis.py

```
import os
import base64
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

# Read the PDF file as base64
with open('./path/to/document.pdf', 'rb') as pdf_file:
    pdf_base64 = base64.b64encode(pdf_file.read()).decode('utf-8')

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': [\
                {\
                    'type': 'text',\
                    'text': 'What is the main topic of this document? Please summarize the key points.'\
                },\
                {\
                    'type': 'file',\
                    'file': {\
                        'data': pdf_base64,\
                        'media_type': 'application/pdf',\
                        'filename': 'document.pdf'\
                    }\
                }\
            ]\
        }\
    ],
    stream=False,
)

print('Assistant:', completion.choices[0].message.content)
print('Tokens used:', completion.usage)
```

### [Tool calls](https://vercel.com/docs/ai-gateway/openai-compat\#tool-calls)

The AI Gateway supports OpenAI-compatible function calling, allowing models to call tools and functions. This follows the same specification as the [OpenAI Function Calling API](https://platform.openai.com/docs/guides/function-calling).

#### [Basic tool calls](https://vercel.com/docs/ai-gateway/openai-compat\#basic-tool-calls)

TypeScriptPython

tool-calls.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const tools: OpenAI.Chat.Completions.ChatCompletionTool[] = [\
  {\
    type: 'function',\
    function: {\
      name: 'get_weather',\
      description: 'Get the current weather in a given location',\
      parameters: {\
        type: 'object',\
        properties: {\
          location: {\
            type: 'string',\
            description: 'The city and state, e.g. San Francisco, CA',\
          },\
          unit: {\
            type: 'string',\
            enum: ['celsius', 'fahrenheit'],\
            description: 'The unit for temperature',\
          },\
        },\
        required: ['location'],\
      },\
    },\
  },\
];

const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: 'What is the weather like in San Francisco?',\
    },\
  ],
  tools: tools,
  tool_choice: 'auto',
  stream: false,
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Tool calls:', completion.choices[0].message.tool_calls);
```

tool-calls.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

tools = [\
    {\
        'type': 'function',\
        'function': {\
            'name': 'get_weather',\
            'description': 'Get the current weather in a given location',\
            'parameters': {\
                'type': 'object',\
                'properties': {\
                    'location': {\
                        'type': 'string',\
                        'description': 'The city and state, e.g. San Francisco, CA'\
                    },\
                    'unit': {\
                        'type': 'string',\
                        'enum': ['celsius', 'fahrenheit'],\
                        'description': 'The unit for temperature'\
                    }\
                },\
                'required': ['location']\
            }\
        }\
    }\
]

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'What is the weather like in San Francisco?'\
        }\
    ],
    tools=tools,
    tool_choice='auto',
    stream=False,
)

print('Assistant:', completion.choices[0].message.content)
print('Tool calls:', completion.choices[0].message.tool_calls)
```

Controlling tool selection: By default, `tool_choice` is set to `'auto'`, allowing the model to decide when to use tools. You can also:

- Set to `'none'` to disable tool calls
- Force a specific tool with: `tool_choice: { type: 'function', function: { name: 'your_function_name' } }`

#### [Tool call response format](https://vercel.com/docs/ai-gateway/openai-compat\#tool-call-response-format)

When the model makes tool calls, the response includes tool call information:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "anthropic/claude-sonnet-4",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": null,\
        "tool_calls": [\
          {\
            "id": "call_123",\
            "type": "function",\
            "function": {\
              "name": "get_weather",\
              "arguments": "{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}"\
            }\
          }\
        ]\
      },\
      "finish_reason": "tool_calls"\
    }\
  ],
  "usage": {
    "prompt_tokens": 82,
    "completion_tokens": 18,
    "total_tokens": 100
  }
}
```

### [Structured outputs](https://vercel.com/docs/ai-gateway/openai-compat\#structured-outputs)

Generate structured JSON responses that conform to a specific schema, ensuring predictable and reliable data formats for your applications.

#### [JSON Schema format](https://vercel.com/docs/ai-gateway/openai-compat\#json-schema-format)

Use the OpenAI standard `json_schema` response format for the most robust structured output experience. This follows the official [OpenAI Structured Outputs specification](https://platform.openai.com/docs/guides/structured-outputs).

Example request

TypeScriptPython

structured-output-json-schema.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const completion = await openai.chat.completions.create({
  model: 'openai/gpt-5',
  messages: [\
    {\
      role: 'user',\
      content: 'Create a product listing for a wireless gaming headset.',\
    },\
  ],
  stream: false,
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'product_listing',
      description: 'A product listing with details and pricing',
      schema: {
        type: 'object',
        properties: {
          name: {
            type: 'string',
            description: 'Product name',
          },
          brand: {
            type: 'string',
            description: 'Brand name',
          },
          price: {
            type: 'number',
            description: 'Price in USD',
          },
          category: {
            type: 'string',
            description: 'Product category',
          },
          description: {
            type: 'string',
            description: 'Product description',
          },
          features: {
            type: 'array',
            items: { type: 'string' },
            description: 'Key product features',
          },
        },
        required: ['name', 'brand', 'price', 'category', 'description'],
        additionalProperties: false,
      },
    },
  },
});

console.log('Assistant:', completion.choices[0].message.content);

// Parse the structured response
const structuredData = JSON.parse(completion.choices[0].message.content);
console.log('Structured Data:', structuredData);
```

structured-output-json-schema.py

```
import os
import json
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='openai/gpt-5',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Create a product listing for a wireless gaming headset.'\
        }\
    ],
    stream=False,
    response_format={
        'type': 'json_schema',
        'json_schema': {
            'name': 'product_listing',
            'description': 'A product listing with details and pricing',
            'schema': {
                'type': 'object',
                'properties': {
                    'name': {
                        'type': 'string',
                        'description': 'Product name'
                    },
                    'brand': {
                        'type': 'string',
                        'description': 'Brand name'
                    },
                    'price': {
                        'type': 'number',
                        'description': 'Price in USD'
                    },
                    'category': {
                        'type': 'string',
                        'description': 'Product category'
                    },
                    'description': {
                        'type': 'string',
                        'description': 'Product description'
                    },
                    'features': {
                        'type': 'array',
                        'items': {'type': 'string'},
                        'description': 'Key product features'
                    }
                },
                'required': ['name', 'brand', 'price', 'category', 'description'],
                'additionalProperties': False
            },
        }
    }
)

print('Assistant:', completion.choices[0].message.content)

# Parse the structured response
structured_data = json.loads(completion.choices[0].message.content)
print('Structured Data:', json.dumps(structured_data, indent=2))
```

Response format

The response contains structured JSON that conforms to your specified schema:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "openai/gpt-5",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "{\"name\":\"SteelSeries Arctis 7P\",\"brand\":\"SteelSeries\",\"price\":149.99,\"category\":\"Gaming Headsets\",\"description\":\"Wireless gaming headset with 7.1 surround sound\",\"features\":[\"Wireless 2.4GHz\",\"7.1 Surround Sound\",\"24-hour battery\",\"Retractable microphone\"]}"\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 45,
    "total_tokens": 70
  }
}
```

#### [JSON Schema parameters](https://vercel.com/docs/ai-gateway/openai-compat\#json-schema-parameters)

- `type`: Must be `"json_schema"`
- `json_schema`: Object containing schema definition
  - `name` (required): Name of the response schema
  - `description` (optional): Human-readable description of the expected output
  - `schema` (required): Valid JSON Schema object defining the structure

#### [Legacy JSON format (alternative)](https://vercel.com/docs/ai-gateway/openai-compat\#legacy-json-format-alternative)

Legacy format: The following format is supported for backward
compatibility. For new implementations, use the `json_schema` format above.

TypeScriptPython

structured-output-legacy.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const completion = await openai.chat.completions.create({
  model: 'openai/gpt-5',
  messages: [\
    {\
      role: 'user',\
      content: 'Create a product listing for a wireless gaming headset.',\
    },\
  ],
  stream: false,
  // @ts-expect-error - Legacy format not in OpenAI types
  response_format: {
    type: 'json',
    name: 'product_listing',
    description: 'A product listing with details and pricing',
    schema: {
      type: 'object',
      properties: {
        name: { type: 'string', description: 'Product name' },
        brand: { type: 'string', description: 'Brand name' },
        price: { type: 'number', description: 'Price in USD' },
        category: { type: 'string', description: 'Product category' },
        description: { type: 'string', description: 'Product description' },
        features: {
          type: 'array',
          items: { type: 'string' },
          description: 'Key product features',
        },
      },
      required: ['name', 'brand', 'price', 'category', 'description'],
    },
  },
});

console.log('Assistant:', completion.choices[0].message.content);
```

structured-output-legacy.py

```
import os
import json
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='openai/gpt-5',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Create a product listing for a wireless gaming headset.'\
        }\
    ],
    stream=False,
    response_format={
        'type': 'json',
        'name': 'product_listing',
        'description': 'A product listing with details and pricing',
        'schema': {
            'type': 'object',
            'properties': {
                'name': {'type': 'string', 'description': 'Product name'},
                'brand': {'type': 'string', 'description': 'Brand name'},
                'price': {'type': 'number', 'description': 'Price in USD'},
                'category': {'type': 'string', 'description': 'Product category'},
                'description': {'type': 'string', 'description': 'Product description'},
                'features': {
                    'type': 'array',
                    'items': {'type': 'string'},
                    'description': 'Key product features'
                }
            },
            'required': ['name', 'brand', 'price', 'category', 'description']
        }
    }
)

print('Assistant:', completion.choices[0].message.content)

# Parse the structured response
structured_data = json.loads(completion.choices[0].message.content)
print('Structured Data:', json.dumps(structured_data, indent=2))
```

#### [Streaming with structured outputs](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-with-structured-outputs)

Both `json_schema` and legacy `json` formats work with streaming responses:

TypeScriptPython

structured-streaming.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const stream = await openai.chat.completions.create({
  model: 'openai/gpt-5',
  messages: [\
    {\
      role: 'user',\
      content: 'Create a product listing for a wireless gaming headset.',\
    },\
  ],
  stream: true,
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'product_listing',
      description: 'A product listing with details and pricing',
      schema: {
        type: 'object',
        properties: {
          name: { type: 'string', description: 'Product name' },
          brand: { type: 'string', description: 'Brand name' },
          price: { type: 'number', description: 'Price in USD' },
          category: { type: 'string', description: 'Product category' },
          description: { type: 'string', description: 'Product description' },
          features: {
            type: 'array',
            items: { type: 'string' },
            description: 'Key product features',
          },
        },
        required: ['name', 'brand', 'price', 'category', 'description'],
        additionalProperties: false,
      },
    },
  },
});

let completeResponse = '';
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
    completeResponse += content;
  }
}

// Parse the complete structured response
const structuredData = JSON.parse(completeResponse);
console.log('\nParsed Product:', structuredData);
```

structured-streaming.py

```
import os
import json
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

stream = client.chat.completions.create(
    model='openai/gpt-5',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Create a product listing for a wireless gaming headset.'\
        }\
    ],
    stream=True,
    response_format={
        'type': 'json_schema',
        'json_schema': {
            'name': 'product_listing',
            'description': 'A product listing with details and pricing',
            'schema': {
                'type': 'object',
                'properties': {
                    'name': {'type': 'string', 'description': 'Product name'},
                    'brand': {'type': 'string', 'description': 'Brand name'},
                    'price': {'type': 'number', 'description': 'Price in USD'},
                    'category': {'type': 'string', 'description': 'Product category'},
                    'description': {'type': 'string', 'description': 'Product description'},
                    'features': {
                        'type': 'array',
                        'items': {'type': 'string'},
                        'description': 'Key product features'
                    }
                },
                'required': ['name', 'brand', 'price', 'category', 'description'],
                'additionalProperties': False
            },
        }
    }
)

complete_response = ''
for chunk in stream:
    if chunk.choices and chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
        print(content, end='', flush=True)
        complete_response += content

# Parse the complete structured response
structured_data = json.loads(complete_response)
print('\nParsed Product:', json.dumps(structured_data, indent=2))
```

Streaming assembly: When using structured outputs with streaming, you'll
need to collect all the content chunks and parse the complete JSON response
once the stream is finished.

### [Reasoning configuration](https://vercel.com/docs/ai-gateway/openai-compat\#reasoning-configuration)

Configure reasoning behavior for models that support extended thinking or chain-of-thought reasoning. The `reasoning` parameter allows you to control how reasoning tokens are generated and returned.

Example request

TypeScript (OpenAI SDK)TypeScript (fetch)Python

reasoning-openai-sdk.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// @ts-expect-error - reasoning parameter not yet in OpenAI types
const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: 'What is the meaning of life? Think before answering.',\
    },\
  ],
  stream: false,
  reasoning: {
    max_tokens: 2000, // Limit reasoning tokens
    enabled: true, // Enable reasoning
  },
});

console.log('Reasoning:', completion.choices[0].message.reasoning);
console.log('Answer:', completion.choices[0].message.content);
console.log(
  'Reasoning tokens:',
  completion.usage.completion_tokens_details?.reasoning_tokens,
);
```

reasoning-fetch.ts

```
const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content: 'What is the meaning of life? Think before answering.',\
        },\
      ],
      stream: false,
      reasoning: {
        max_tokens: 2000,
        enabled: true,
      },
    }),
  },
);

const completion = await response.json();
console.log('Reasoning:', completion.choices[0].message.reasoning);
console.log('Answer:', completion.choices[0].message.content);
```

reasoning.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'What is the meaning of life? Think before answering.'\
        }\
    ],
    stream=False,
    extra_body={
        'reasoning': {
            'max_tokens': 2000,
            'enabled': True
        }
    }
)

print('Reasoning:', completion.choices[0].message.reasoning)
print('Answer:', completion.choices[0].message.content)
print('Reasoning tokens:', completion.usage.completion_tokens_details.reasoning_tokens)
```

#### [Reasoning parameters](https://vercel.com/docs/ai-gateway/openai-compat\#reasoning-parameters)

The `reasoning` object supports the following parameters:

- `enabled` (boolean, optional): Enable reasoning output. When `true`, the model will provide its reasoning process.
- `max_tokens` (number, optional): Maximum number of tokens to allocate for reasoning. This helps control costs and response times. Cannot be used with `effort`.
- `effort` (string, optional): Control reasoning effort level. Accepts `'low'`, `'medium'`, or `'high'`. Cannot be used with `max_tokens`.
- `exclude` (boolean, optional): When `true`, excludes reasoning content from the response but still generates it internally. Useful for reducing response payload size.

Mutually exclusive parameters: You cannot specify both `effort` and
`max_tokens` in the same request. Choose one based on your use case.

#### [Response format with reasoning](https://vercel.com/docs/ai-gateway/openai-compat\#response-format-with-reasoning)

When reasoning is enabled, the response includes reasoning content:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "anthropic/claude-sonnet-4",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "The meaning of life is a deeply personal question...",\
        "reasoning": "Let me think about this carefully. The question asks about..."\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 150,
    "total_tokens": 165,
    "completion_tokens_details": {
      "reasoning_tokens": 50
    }
  }
}
```

#### [Streaming with reasoning](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-with-reasoning)

Reasoning content is streamed incrementally in the `delta.reasoning` field:

TypeScriptPython

reasoning-streaming.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// @ts-expect-error - reasoning parameter not yet in OpenAI types
const stream = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content: 'What is the meaning of life? Think before answering.',\
    },\
  ],
  stream: true,
  reasoning: {
    enabled: true,
  },
});

for await (const chunk of stream) {
  const delta = chunk.choices[0]?.delta;

  // Handle reasoning content
  if (delta?.reasoning) {
    process.stdout.write(`[Reasoning] ${delta.reasoning}`);
  }

  // Handle regular content
  if (delta?.content) {
    process.stdout.write(delta.content);
  }
}
```

reasoning-streaming.py

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

stream = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'What is the meaning of life? Think before answering.'\
        }\
    ],
    stream=True,
    extra_body={
        'reasoning': {
            'enabled': True
        }
    }
)

for chunk in stream:
    if chunk.choices and chunk.choices[0].delta:
        delta = chunk.choices[0].delta

        # Handle reasoning content
        if hasattr(delta, 'reasoning') and delta.reasoning:
            print(f"[Reasoning] {delta.reasoning}", end='', flush=True)

        # Handle regular content
        if hasattr(delta, 'content') and delta.content:
            print(delta.content, end='', flush=True)
```

#### [Preserving reasoning details across providers](https://vercel.com/docs/ai-gateway/openai-compat\#preserving-reasoning-details-across-providers)

The AI Gateway preserves reasoning details from models across interactions,
normalizing the different formats used by OpenAI, Anthropic, and other providers into a consistent structure.
This allows you to switch between models without rewriting your conversation management logic.

This is particularly useful during tool calling workflows where the model needs to
resume its thought process after receiving tool results.

Controlling reasoning details

When `reasoning.enabled` is `true` (or when `reasoning.exclude` is not set),
responses include a `reasoning_details` array alongside the standard `reasoning` text field.
This structured field captures cryptographic signatures, encrypted content, and other verification
data that providers include with their reasoning output.

Each detail object contains:

- `type`: one or more of the below, depending on the provider and model
  - `'reasoning.text'`: Contains the actual reasoning content as plain text in the `text` field. May include a `signature` field (Anthropic models) for cryptographic verification.
  - `'reasoning.encrypted'`: Contains encrypted or redacted reasoning content in the `data` field. Used by OpenAI models when reasoning is protected, or by Anthropic models when thinking is redacted. Preserves the encrypted payload for verification purposes.
  - `'reasoning.summary'`: Contains a condensed version of the reasoning process in the `summary` field. Used by OpenAI models to provide a readable summary alongside encrypted reasoning.
- `id` (optional): Unique identifier for the reasoning block, used for tracking and correlation
- `format`: Provider format identifier - `'openai-responses-v1'`, `'anthropic-claude-v1'`, or `'unknown'`
- `index` (optional): Position in the reasoning sequence (for responses with multiple reasoning blocks)

Example response with reasoning details

For Anthropic models:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "anthropic/claude-sonnet-4",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "The meaning of life is a deeply personal question...",\
        "reasoning": "Let me think about this carefully. The question asks about...",\
        "reasoning_details": [\
          {\
            "type": "reasoning.text",\
            "text": "Let me think about this carefully. The question asks about...",\
            "signature": "anthropic-signature-xyz",\
            "format": "anthropic-claude-v1",\
            "index": 0\
          }\
        ]\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 150,
    "total_tokens": 165,
    "completion_tokens_details": {
      "reasoning_tokens": 50
    }
  }
}
```

For OpenAI models (returns both summary and encrypted):

```
{
  "id": "chatcmpl-456",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "openai/o3-mini",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "The answer is 42.",\
        "reasoning": "Let me calculate this step by step...",\
        "reasoning_details": [\
          {\
            "type": "reasoning.summary",\
            "summary": "Let me calculate this step by step...",\
            "format": "openai-responses-v1",\
            "index": 0\
          },\
          {\
            "type": "reasoning.encrypted",\
            "data": "encrypted_reasoning_content_xyz",\
            "format": "openai-responses-v1",\
            "index": 1\
          }\
        ]\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 150,
    "total_tokens": 165,
    "completion_tokens_details": {
      "reasoning_tokens": 50
    }
  }
}
```

Streaming reasoning details

When streaming, reasoning details are delivered incrementally in `delta.reasoning_details`:

For Anthropic models:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "anthropic/claude-sonnet-4",
  "choices": [\
    {\
      "index": 0,\
      "delta": {\
        "reasoning": "Let me think.",\
        "reasoning_details": [\
          {\
            "type": "reasoning.text",\
            "text": "Let me think.",\
            "signature": "anthropic-signature-xyz",\
            "format": "anthropic-claude-v1",\
            "index": 0\
          }\
        ]\
      },\
      "finish_reason": null\
    }\
  ]
}
```

For OpenAI models (summary chunks during reasoning, then encrypted at end):

```
{
  "id": "chatcmpl-456",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "openai/o3-mini",
  "choices": [\
    {\
      "index": 0,\
      "delta": {\
        "reasoning": "Step 1:",\
        "reasoning_details": [\
          {\
            "type": "reasoning.summary",\
            "summary": "Step 1:",\
            "format": "openai-responses-v1",\
            "index": 0\
          }\
        ]\
      },\
      "finish_reason": null\
    }\
  ]
}
```

#### [Provider-specific behavior](https://vercel.com/docs/ai-gateway/openai-compat\#provider-specific-behavior)

The AI Gateway automatically maps reasoning parameters to each provider's native format:

- OpenAI: Maps `effort` to `reasoningEffort` and controls summary detail
- Anthropic: Maps `max_tokens` to thinking budget tokens
- Google: Maps to `thinkingConfig` with budget and visibility settings
- Groq: Maps `exclude` to control reasoning format (hidden/parsed)
- xAI: Maps `effort` to reasoning effort levels
- Other providers: Generic mapping applied for compatibility

Automatic extraction: For models that don't natively support reasoning
output, the gateway automatically extracts reasoning
from `<think>` tags in the response.

### [Provider options](https://vercel.com/docs/ai-gateway/openai-compat\#provider-options)

The AI Gateway can route your requests across multiple AI providers for better reliability and performance. You can control which providers are used and in what order through the `providerOptions` parameter.

Example request

TypeScriptPython

provider-options.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// @ts-expect-error
const completion = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content:\
        'Tell me the history of the San Francisco Mission-style burrito in two paragraphs.',\
    },\
  ],
  stream: false,
  // Provider options for gateway routing preferences
  providerOptions: {
    gateway: {
      order: ['vertex', 'anthropic'], // Try Vertex AI first, then Anthropic
    },
  },
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Tokens used:', completion.usage);
```

provider-options.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Tell me the history of the San Francisco Mission-style burrito in two paragraphs.'\
        }\
    ],
    stream=False,
    # Provider options for gateway routing preferences
    extra_body={
        'providerOptions': {
            'gateway': {
                'order': ['vertex', 'anthropic']  # Try Vertex AI first, then Anthropic
            }
        }
    }
)

print('Assistant:', completion.choices[0].message.content)
print('Tokens used:', completion.usage)
```

Provider routing: In this example, the gateway will first attempt to use
Vertex AI to serve the Claude model. If Vertex AI is unavailable or fails, it
will fall back to Anthropic. Other providers are still available but will only
be used after the specified providers.

#### [Model fallbacks](https://vercel.com/docs/ai-gateway/openai-compat\#model-fallbacks)

You can specify fallback models that will be tried in order if the primary model fails. There are two ways to do this:

###### Option 1: Direct `models` field

The simplest way is to use the `models` field directly at the top level of your request:

TypeScriptPython

model-fallbacks.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const completion = await openai.chat.completions.create({
  model: 'openai/gpt-4o', // Primary model
  // @ts-ignore - models is a gateway extension
  models: ['openai/gpt-5-nano', 'gemini-2.0-flash'], // Fallback models
  messages: [\
    {\
      role: 'user',\
      content: 'Write a haiku about TypeScript.',\
    },\
  ],
  stream: false,
});

console.log('Assistant:', completion.choices[0].message.content);

// Check which model was actually used
console.log('Model used:', completion.model);
```

model-fallbacks.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='openai/gpt-4o',  # Primary model
    messages=[\
        {\
            'role': 'user',\
            'content': 'Write a haiku about TypeScript.'\
        }\
    ],
    stream=False,
    # models is a gateway extension for fallback models
    extra_body={
        'models': ['openai/gpt-5-nano', 'gemini-2.0-flash']  # Fallback models
    }
)

print('Assistant:', completion.choices[0].message.content)

# Check which model was actually used
print('Model used:', completion.model)
```

###### Option 2: Via provider options

Alternatively, you can specify model fallbacks through the `providerOptions.gateway.models` field:

TypeScriptPython

model-fallbacks-provider-options.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// @ts-expect-error
const completion = await openai.chat.completions.create({
  model: 'openai/gpt-4o', // Primary model
  messages: [\
    {\
      role: 'user',\
      content: 'Write a haiku about TypeScript.',\
    },\
  ],
  stream: false,
  // Model fallbacks via provider options
  providerOptions: {
    gateway: {
      models: ['openai/gpt-5-nano', 'gemini-2.0-flash'], // Fallback models
    },
  },
});

console.log('Assistant:', completion.choices[0].message.content);
console.log('Model used:', completion.model);
```

model-fallbacks-provider-options.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='openai/gpt-4o',  # Primary model
    messages=[\
        {\
            'role': 'user',\
            'content': 'Write a haiku about TypeScript.'\
        }\
    ],
    stream=False,
    # Model fallbacks via provider options
    extra_body={
        'providerOptions': {
            'gateway': {
                'models': ['openai/gpt-5-nano', 'gemini-2.0-flash']  # Fallback models
            }
        }
    }
)

print('Assistant:', completion.choices[0].message.content)
print('Model used:', completion.model)
```

Which approach to use: Both methods achieve the same result. Use the
direct `models` field (Option 1) for simplicity, or use `providerOptions`
(Option 2) if you're already using provider options for other configurations.

Both configurations will:

1. Try the primary model (`openai/gpt-4o`) first
2. If it fails, try `openai/gpt-5-nano`
3. If that also fails, try `gemini-2.0-flash`
4. Return the result from the first model that succeeds

#### [Streaming with provider options](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-with-provider-options)

Provider options work with streaming requests as well:

TypeScriptPython

streaming-provider-options.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

// @ts-expect-error
const stream = await openai.chat.completions.create({
  model: 'anthropic/claude-sonnet-4',
  messages: [\
    {\
      role: 'user',\
      content:\
        'Tell me the history of the San Francisco Mission-style burrito in two paragraphs.',\
    },\
  ],
  stream: true,
  providerOptions: {
    gateway: {
      order: ['vertex', 'anthropic'],
    },
  },
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
  }
}
```

streaming-provider-options.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

stream = client.chat.completions.create(
    model='anthropic/claude-sonnet-4',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Tell me the history of the San Francisco Mission-style burrito in two paragraphs.'\
        }\
    ],
    stream=True,
    extra_body={
        'providerOptions': {
            'gateway': {
                'order': ['vertex', 'anthropic']
            }
        }
    }
)

for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end='', flush=True)
```

For more details about available providers and advanced provider configuration, see the [Provider Options documentation](https://vercel.com/docs/ai-gateway/provider-options).

### [Parameters](https://vercel.com/docs/ai-gateway/openai-compat\#parameters)

The chat completions endpoint supports the following parameters:

#### [Required parameters](https://vercel.com/docs/ai-gateway/openai-compat\#required-parameters)

- `model` (string): The model to use for the completion (e.g., `anthropic/claude-sonnet-4`)
- `messages` (array): Array of message objects with `role` and `content` fields

#### [Optional parameters](https://vercel.com/docs/ai-gateway/openai-compat\#optional-parameters)

- `stream` (boolean): Whether to stream the response. Defaults to `false`
- `temperature` (number): Controls randomness in the output. Range: 0-2
- `max_tokens` (integer): Maximum number of tokens to generate
- `top_p` (number): Nucleus sampling parameter. Range: 0-1
- `frequency_penalty` (number): Penalty for frequent tokens. Range: -2 to 2
- `presence_penalty` (number): Penalty for present tokens. Range: -2 to 2
- `stop` (string or array): Stop sequences for the generation
- `tools` (array): Array of tool definitions for function calling
- `tool_choice` (string or object): Controls which tools are called (`auto`, `none`, or specific function)
- `providerOptions` (object): [Provider routing and configuration options](https://vercel.com/docs/ai-gateway/openai-compat#provider-options)
- `response_format` (object): Controls the format of the model's response
  - For OpenAI standard format: `{ type: "json_schema", json_schema: { name, schema, strict?, description? } }`
  - For legacy format: `{ type: "json", schema?, name?, description? }`
  - For plain text: `{ type: "text" }`
  - See [Structured outputs](https://vercel.com/docs/ai-gateway/openai-compat#structured-outputs) for detailed examples

### [Message format](https://vercel.com/docs/ai-gateway/openai-compat\#message-format)

Messages support different content types:

#### [Text messages](https://vercel.com/docs/ai-gateway/openai-compat\#text-messages)

```
{
  "role": "user",
  "content": "Hello, how are you?"
}
```

#### [Multimodal messages](https://vercel.com/docs/ai-gateway/openai-compat\#multimodal-messages)

```
{
  "role": "user",
  "content": [\
    { "type": "text", "text": "What's in this image?" },\
    {\
      "type": "image_url",\
      "image_url": {\
        "url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD..."\
      }\
    }\
  ]
}
```

#### [File messages](https://vercel.com/docs/ai-gateway/openai-compat\#file-messages)

```
{
  "role": "user",
  "content": [\
    { "type": "text", "text": "Summarize this document" },\
    {\
      "type": "file",\
      "file": {\
        "data": "JVBERi0xLjQKJcfsj6IKNSAwIG9iago8PAovVHlwZSAvUGFnZQo...",\
        "media_type": "application/pdf",\
        "filename": "document.pdf"\
      }\
    }\
  ]
}
```

## [Image generation](https://vercel.com/docs/ai-gateway/openai-compat\#image-generation)

Generate images using AI models that support multimodal output through the OpenAI-compatible API. This feature allows you to create images alongside text responses using models like Google's Gemini 2.5 Flash Image.

Endpoint

`POST /chat/completions
`

Parameters

To enable image generation, include the `modalities` parameter in your request:

- `modalities` (array): Array of strings specifying the desired output modalities. Use `['text', 'image']` for both text and image generation, or `['image']` for image-only generation.

Example requests

TypeScriptPython

image-generation.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const completion = await openai.chat.completions.create({
  model: 'google/gemini-2.5-flash-image-preview',
  messages: [\
    {\
      role: 'user',\
      content:\
        'Generate a beautiful sunset over mountains and describe the scene.',\
    },\
  ],
  // @ts-expect-error - modalities not yet in OpenAI types but supported by gateway
  modalities: ['text', 'image'],
  stream: false,
});

const message = completion.choices[0].message;

// Text content is always a string
console.log('Text:', message.content);

// Images are in a separate array
if (message.images && Array.isArray(message.images)) {
  console.log(`Generated ${message.images.length} images:`);
  for (const [index, img] of message.images.entries()) {
    if (img.type === 'image_url' && img.image_url) {
      console.log(`Image ${index + 1}:`, {
        size: img.image_url.url?.length || 0,
        preview: `${img.image_url.url?.substring(0, 50)}...`,
      });
    }
  }
}
```

image-generation.py

```
import os
from openai import OpenAI

api_key = os.getenv('AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')

client = OpenAI(
    api_key=api_key,
    base_url='https://ai-gateway.vercel.sh/v1'
)

completion = client.chat.completions.create(
    model='google/gemini-2.5-flash-image-preview',
    messages=[\
        {\
            'role': 'user',\
            'content': 'Generate a beautiful sunset over mountains and describe the scene.'\
        }\
    ],
    # Note: modalities parameter is not yet in OpenAI Python types but supported by our gateway
    extra_body={'modalities': ['text', 'image']},
    stream=False,
)

message = completion.choices[0].message

# Text content is always a string
print(f"Text: {message.content}")

# Images are in a separate array
if hasattr(message, 'images') and message.images:
    print(f"Generated {len(message.images)} images:")
    for i, img in enumerate(message.images):
        if img.get('type') == 'image_url' and img.get('image_url'):
            image_url = img['image_url']['url']
            data_size = len(image_url) if image_url else 0
            print(f"Image {i+1}: size: {data_size} chars")
            print(f"Preview: {image_url[:50]}...")

print(f'Tokens used: {completion.usage}')
```

Response format

When image generation is enabled, the response separates text content from generated images:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "google/gemini-2.5-flash-image-preview",
  "choices": [\
    {\
      "index": 0,\
      "message": {\
        "role": "assistant",\
        "content": "Here's a beautiful sunset scene over the mountains...",\
        "images": [\
          {\
            "type": "image_url",\
            "image_url": {\
              "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="\
            }\
          }\
        ]\
      },\
      "finish_reason": "stop"\
    }\
  ],
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 28,
    "total_tokens": 43
  }
}
```

### [Response structure details](https://vercel.com/docs/ai-gateway/openai-compat\#response-structure-details)

- `content`: Contains the text description as a string
- `images`: Array of generated images, each with:
  - `type`: Always `"image_url"`
  - `image_url.url`: Base64-encoded data URI of the generated image

### [Streaming responses](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-responses)

For streaming requests, images are delivered in delta chunks:

```
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "google/gemini-2.5-flash-image-preview",
  "choices": [\
    {\
      "index": 0,\
      "delta": {\
        "images": [\
          {\
            "type": "image_url",\
            "image_url": {\
              "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="\
            }\
          }\
        ]\
      },\
      "finish_reason": null\
    }\
  ]
}
```

### [Handling streaming image responses](https://vercel.com/docs/ai-gateway/openai-compat\#handling-streaming-image-responses)

When processing streaming responses, check for both text content and images in each delta:

TypeScriptPython

streaming-images.ts

```
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.AI_GATEWAY_API_KEY,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const stream = await openai.chat.completions.create({
  model: 'google/gemini-2.5-flash-image-preview',
  messages: [{ role: 'user', content: 'Generate a sunset image' }],
  // @ts-expect-error - modalities not yet in OpenAI types
  modalities: ['text', 'image'],
  stream: true,
});

for await (const chunk of stream) {
  const delta = chunk.choices[0]?.delta;

  // Handle text content
  if (delta?.content) {
    process.stdout.write(delta.content);
  }

  // Handle images
  if (delta?.images) {
    for (const img of delta.images) {
      if (img.type === 'image_url' && img.image_url) {
        console.log(`\n[Image received: ${img.image_url.url.length} chars]`);
      }
    }
  }
}
```

streaming-images.py

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv('AI_GATEWAY_API_KEY'),
    base_url='https://ai-gateway.vercel.sh/v1'
)

stream = client.chat.completions.create(
    model='google/gemini-2.5-flash-image-preview',
    messages=[{'role': 'user', 'content': 'Generate a sunset image'}],
    extra_body={'modalities': ['text', 'image']},
    stream=True,
)

for chunk in stream:
    if chunk.choices and chunk.choices[0].delta:
        delta = chunk.choices[0].delta

        # Handle text content
        if hasattr(delta, 'content') and delta.content:
            print(delta.content, end='', flush=True)

        # Handle images
        if hasattr(delta, 'images') and delta.images:
            for img in delta.images:
                if img.get('type') == 'image_url' and img.get('image_url'):
                    image_url = img['image_url']['url']
                    print(f"\n[Image received: {len(image_url)} chars]")
```

Image generation support: Currently, image generation is supported by
Google's Gemini 2.5 Flash Image model. The generated images are returned as
base64-encoded data URIs in the response. For more detailed information about
image generation capabilities, see the [Image Generation\\
documentation](https://vercel.com/docs/ai-gateway/image-generation).

## [Embeddings](https://vercel.com/docs/ai-gateway/openai-compat\#embeddings)

Generate vector embeddings from input text for semantic search, similarity matching, and retrieval-augmented generation (RAG).

Endpoint

`POST /embeddings
`

Example request

TypeScriptPython

embeddings.ts

```
import OpenAI from 'openai';

const apiKey = process.env.AI_GATEWAY_API_KEY || process.env.VERCEL_OIDC_TOKEN;

const openai = new OpenAI({
  apiKey,
  baseURL: 'https://ai-gateway.vercel.sh/v1',
});

const response = await openai.embeddings.create({
  model: 'openai/text-embedding-3-small',
  input: 'Sunny day at the beach',
});

console.log(response.data[0].embedding);
```

embeddings.py

```
import os
from openai import OpenAI

api_key = os.getenv("AI_GATEWAY_API_KEY") or os.getenv("VERCEL_OIDC_TOKEN")

client = OpenAI(
    api_key=api_key,
    base_url="https://ai-gateway.vercel.sh/v1",
)

response = client.embeddings.create(
    model="openai/text-embedding-3-small",
    input="Sunny day at the beach",
)

print(response.data[0].embedding)
```

Response format

```
{
  "object": "list",
  "data": [\
    {\
      "object": "embedding",\
      "index": 0,\
      "embedding": [-0.0038, 0.021, ...]\
    },\
  ],
  "model": "openai/text-embedding-3-small",
  "usage": {
    "prompt_tokens": 6,
    "total_tokens": 6
  },
  "providerMetadata": {
    "gateway": {
      "routing": { ... }, // Detailed routing info
      "cost": "0.00000012"
    }
  }
}
```

Dimensions parameter

You can set the root-level `dimensions` field (from the [OpenAI Embeddings API spec](https://platform.openai.com/docs/api-reference/embeddings/create)) and the gateway will auto-map it to each provider's expected field; `providerOptions.[provider]` still passes through as-is and isn't required for `dimensions` to work.

TypeScriptPython

embeddings-dimensions.ts

```
const response = await openai.embeddings.create({
  model: 'openai/text-embedding-3-small',
  input: 'Sunny day at the beach',
  dimensions: 768,
});
```

embeddings-dimensions.py

```
response = client.embeddings.create(
    model='openai/text-embedding-3-small',
    input='Sunny day at the beach',
    dimensions=768,
)
```

## [Error handling](https://vercel.com/docs/ai-gateway/openai-compat\#error-handling)

The API returns standard HTTP status codes and error responses:

### [Common error codes](https://vercel.com/docs/ai-gateway/openai-compat\#common-error-codes)

- `400 Bad Request`: Invalid request parameters
- `401 Unauthorized`: Invalid or missing authentication
- `403 Forbidden`: Insufficient permissions
- `404 Not Found`: Model or endpoint not found
- `429 Too Many Requests`: Rate limit exceeded
- `500 Internal Server Error`: Server error

### [Error response format](https://vercel.com/docs/ai-gateway/openai-compat\#error-response-format)

```
{
  "error": {
    "message": "Invalid request: missing required parameter 'model'",
    "type": "invalid_request_error",
    "param": "model",
    "code": "missing_parameter"
  }
}
```

## [Direct REST API usage](https://vercel.com/docs/ai-gateway/openai-compat\#direct-rest-api-usage)

If you prefer to use the AI Gateway API directly without the OpenAI client libraries, you can make HTTP requests using any HTTP client. Here are examples using `curl` and JavaScript's `fetch` API:

### [List models](https://vercel.com/docs/ai-gateway/openai-compat\#list-models)

cURLJavaScript

list-models.sh

```
curl -X GET "https://ai-gateway.vercel.sh/v1/models" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json"
```

list-models.js

```
const response = await fetch('https://ai-gateway.vercel.sh/v1/models', {
  method: 'GET',
  headers: {
    Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
    'Content-Type': 'application/json',
  },
});

const models = await response.json();
console.log(models);
```

### [Basic chat completion](https://vercel.com/docs/ai-gateway/openai-compat\#basic-chat-completion)

cURLJavaScript

chat-completion.sh

```
curl -X POST "https://ai-gateway.vercel.sh/v1/chat/completions" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4",
    "messages": [\
      {\
        "role": "user",\
        "content": "Write a one-sentence bedtime story about a unicorn."\
      }\
    ],
    "stream": false
  }'
```

chat-completion.js

```
const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content: 'Write a one-sentence bedtime story about a unicorn.',\
        },\
      ],
      stream: false,
    }),
  },
);

const result = await response.json();
console.log(result);
```

### [Streaming chat completion](https://vercel.com/docs/ai-gateway/openai-compat\#streaming-chat-completion)

cURLJavaScript

streaming-chat.sh

```
curl -X POST "https://ai-gateway.vercel.sh/v1/chat/completions" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4",
    "messages": [\
      {\
        "role": "user",\
        "content": "Write a one-sentence bedtime story about a unicorn."\
      }\
    ],
    "stream": true
  }' \
  --no-buffer
```

streaming-chat.js

```
const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content: 'Write a one-sentence bedtime story about a unicorn.',\
        },\
      ],
      stream: true,
    }),
  },
);

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value);
  const lines = chunk.split('\n');

  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = line.slice(6);
      if (data === '[DONE]') {
        console.log('Stream complete');
        break;
      } else if (data.trim()) {
        const parsed = JSON.parse(data);
        const content = parsed.choices?.[0]?.delta?.content;
        if (content) {
          process.stdout.write(content);
        }
      }
    }
  }
}
```

### [Image analysis](https://vercel.com/docs/ai-gateway/openai-compat\#image-analysis)

cURLJavaScript

image-analysis.sh

```
# First, convert your image to base64
IMAGE_BASE64=$(base64 -i ./path/to/image.png)

curl -X POST "https://ai-gateway.vercel.sh/v1/chat/completions" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4",
    "messages": [\
      {\
        "role": "user",\
        "content": [\
          {\
            "type": "text",\
            "text": "Describe this image in detail."\
          },\
          {\
            "type": "image_url",\
            "image_url": {\
              "url": "data:image/png;base64,'"$IMAGE_BASE64"'",\
              "detail": "auto"\
            }\
          }\
        ]\
      }\
    ],
    "stream": false
  }'
```

image-analysis.js

```
import fs from 'node:fs';

// Read the image file as base64
const imageBuffer = fs.readFileSync('./path/to/image.png');
const imageBase64 = imageBuffer.toString('base64');

const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content: [\
            { type: 'text', text: 'Describe this image in detail.' },\
            {\
              type: 'image_url',\
              image_url: {\
                url: `data:image/png;base64,${imageBase64}`,\
                detail: 'auto',\
              },\
            },\
          ],\
        },\
      ],
      stream: false,
    }),
  },
);

const result = await response.json();
console.log(result);
```

### [Tool calls](https://vercel.com/docs/ai-gateway/openai-compat\#tool-calls)

cURLJavaScript

tool-calls.sh

```
curl -X POST "https://ai-gateway.vercel.sh/v1/chat/completions" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4",
    "messages": [\
      {\
        "role": "user",\
        "content": "What is the weather like in San Francisco?"\
      }\
    ],
    "tools": [\
      {\
        "type": "function",\
        "function": {\
          "name": "get_weather",\
          "description": "Get the current weather in a given location",\
          "parameters": {\
            "type": "object",\
            "properties": {\
              "location": {\
                "type": "string",\
                "description": "The city and state, e.g. San Francisco, CA"\
              },\
              "unit": {\
                "type": "string",\
                "enum": ["celsius", "fahrenheit"],\
                "description": "The unit for temperature"\
              }\
            },\
            "required": ["location"]\
          }\
        }\
      }\
    ],
    "tool_choice": "auto",
    "stream": false
  }'
```

tool-calls.js

```
const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content: 'What is the weather like in San Francisco?',\
        },\
      ],
      tools: [\
        {\
          type: 'function',\
          function: {\
            name: 'get_weather',\
            description: 'Get the current weather in a given location',\
            parameters: {\
              type: 'object',\
              properties: {\
                location: {\
                  type: 'string',\
                  description: 'The city and state, e.g. San Francisco, CA',\
                },\
                unit: {\
                  type: 'string',\
                  enum: ['celsius', 'fahrenheit'],\
                  description: 'The unit for temperature',\
                },\
              },\
              required: ['location'],\
            },\
          },\
        },\
      ],
      tool_choice: 'auto',
      stream: false,
    }),
  },
);

const result = await response.json();
console.log(result);
```

### [Provider options](https://vercel.com/docs/ai-gateway/openai-compat\#provider-options)

cURLJavaScript

provider-options.sh

```
curl -X POST "https://ai-gateway.vercel.sh/v1/chat/completions" \
  -H "Authorization: Bearer $AI_GATEWAY_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4",
    "messages": [\
      {\
        "role": "user",\
        "content": "Tell me the history of the San Francisco Mission-style burrito in two paragraphs."\
      }\
    ],
    "stream": false,
    "providerOptions": {
      "gateway": {
        "order": ["vertex", "anthropic"]
      }
    }
  }'
```

provider-options.js

```
const response = await fetch(
  'https://ai-gateway.vercel.sh/v1/chat/completions',
  {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${process.env.AI_GATEWAY_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-sonnet-4',
      messages: [\
        {\
          role: 'user',\
          content:\
            'Tell me the history of the San Francisco Mission-style burrito in two paragraphs.',\
        },\
      ],
      stream: false,
      providerOptions: {
        gateway: {
          order: ['vertex', 'anthropic'], // Try Vertex AI first, then Anthropic
        },
      },
    }),
  },
);

const result = await response.json();
console.log(result);
```

* * *

[Previous\\
\\
Provider Options](https://vercel.com/docs/ai-gateway/provider-options)

[Next\\
\\
Authentication](https://vercel.com/docs/ai-gateway/authentication)

Was this helpful?

supported.

Send

On this page

- [Base URL](https://vercel.com/docs/ai-gateway/openai-compat#base-url)
- [Authentication](https://vercel.com/docs/ai-gateway/openai-compat#authentication)
- [Supported endpoints](https://vercel.com/docs/ai-gateway/openai-compat#supported-endpoints)
- [Integration with existing tools](https://vercel.com/docs/ai-gateway/openai-compat#integration-with-existing-tools)
- [OpenAI client libraries](https://vercel.com/docs/ai-gateway/openai-compat#openai-client-libraries)
- [AI SDK 4](https://vercel.com/docs/ai-gateway/openai-compat#ai-sdk-4)
- [List models](https://vercel.com/docs/ai-gateway/openai-compat#list-models)
- [Retrieve model](https://vercel.com/docs/ai-gateway/openai-compat#retrieve-model)
- [Chat completions](https://vercel.com/docs/ai-gateway/openai-compat#chat-completions)
- [Basic chat completion](https://vercel.com/docs/ai-gateway/openai-compat#basic-chat-completion)
- [Streaming chat completion](https://vercel.com/docs/ai-gateway/openai-compat#streaming-chat-completion)
- [Streaming response format](https://vercel.com/docs/ai-gateway/openai-compat#streaming-response-format)
- [Image attachments](https://vercel.com/docs/ai-gateway/openai-compat#image-attachments)
- [PDF attachments](https://vercel.com/docs/ai-gateway/openai-compat#pdf-attachments)
- [Tool calls](https://vercel.com/docs/ai-gateway/openai-compat#tool-calls)
- [Basic tool calls](https://vercel.com/docs/ai-gateway/openai-compat#basic-tool-calls)
- [Tool call response format](https://vercel.com/docs/ai-gateway/openai-compat#tool-call-response-format)
- [Structured outputs](https://vercel.com/docs/ai-gateway/openai-compat#structured-outputs)
- [JSON Schema format](https://vercel.com/docs/ai-gateway/openai-compat#json-schema-format)
- [JSON Schema parameters](https://vercel.com/docs/ai-gateway/openai-compat#json-schema-parameters)
- [Legacy JSON format (alternative)](https://vercel.com/docs/ai-gateway/openai-compat#legacy-json-format-alternative)
- [Streaming with structured outputs](https://vercel.com/docs/ai-gateway/openai-compat#streaming-with-structured-outputs)
- [Reasoning configuration](https://vercel.com/docs/ai-gateway/openai-compat#reasoning-configuration)
- [Reasoning parameters](https://vercel.com/docs/ai-gateway/openai-compat#reasoning-parameters)
- [Response format with reasoning](https://vercel.com/docs/ai-gateway/openai-compat#response-format-with-reasoning)
- [Streaming with reasoning](https://vercel.com/docs/ai-gateway/openai-compat#streaming-with-reasoning)
- [Preserving reasoning details across providers](https://vercel.com/docs/ai-gateway/openai-compat#preserving-reasoning-details-across-providers)
- [Provider-specific behavior](https://vercel.com/docs/ai-gateway/openai-compat#provider-specific-behavior)
- [Provider options](https://vercel.com/docs/ai-gateway/openai-compat#provider-options)
- [Model fallbacks](https://vercel.com/docs/ai-gateway/openai-compat#model-fallbacks)
- Option 1: Direct models field
- Option 2: Via provider options
- [Streaming with provider options](https://vercel.com/docs/ai-gateway/openai-compat#streaming-with-provider-options)
- [Parameters](https://vercel.com/docs/ai-gateway/openai-compat#parameters)
- [Required parameters](https://vercel.com/docs/ai-gateway/openai-compat#required-parameters)
- [Optional parameters](https://vercel.com/docs/ai-gateway/openai-compat#optional-parameters)
- [Message format](https://vercel.com/docs/ai-gateway/openai-compat#message-format)
- [Text messages](https://vercel.com/docs/ai-gateway/openai-compat#text-messages)
- [Multimodal messages](https://vercel.com/docs/ai-gateway/openai-compat#multimodal-messages)
- [File messages](https://vercel.com/docs/ai-gateway/openai-compat#file-messages)
- [Image generation](https://vercel.com/docs/ai-gateway/openai-compat#image-generation)
- [Response structure details](https://vercel.com/docs/ai-gateway/openai-compat#response-structure-details)
- [Streaming responses](https://vercel.com/docs/ai-gateway/openai-compat#streaming-responses)
- [Handling streaming image responses](https://vercel.com/docs/ai-gateway/openai-compat#handling-streaming-image-responses)
- [Embeddings](https://vercel.com/docs/ai-gateway/openai-compat#embeddings)
- [Error handling](https://vercel.com/docs/ai-gateway/openai-compat#error-handling)
- [Common error codes](https://vercel.com/docs/ai-gateway/openai-compat#common-error-codes)
- [Error response format](https://vercel.com/docs/ai-gateway/openai-compat#error-response-format)
- [Direct REST API usage](https://vercel.com/docs/ai-gateway/openai-compat#direct-rest-api-usage)
- [List models](https://vercel.com/docs/ai-gateway/openai-compat#list-models)
- [Basic chat completion](https://vercel.com/docs/ai-gateway/openai-compat#basic-chat-completion)
- [Streaming chat completion](https://vercel.com/docs/ai-gateway/openai-compat#streaming-chat-completion)
- [Image analysis](https://vercel.com/docs/ai-gateway/openai-compat#image-analysis)
- [Tool calls](https://vercel.com/docs/ai-gateway/openai-compat#tool-calls)
- [Provider options](https://vercel.com/docs/ai-gateway/openai-compat#provider-options)

Copy pageGive feedbackAsk AI about this page

## Products

- [AI](https://vercel.com/ai)
- [Enterprise](https://vercel.com/enterprise)
- [Fluid Compute](https://vercel.com/fluid)
- [Next.js](https://vercel.com/solutions/nextjs)
- [Observability](https://vercel.com/products/observability)
- [Previews](https://vercel.com/products/previews)
- [Rendering](https://vercel.com/products/rendering)
- [Security](https://vercel.com/security)
- [Turbo](https://vercel.com/solutions/turborepo)
- [Domains](https://vercel.com/domains)
- [v0](https://v0.app/)

## Resources

- [Community](https://community.vercel.com/)
- [Docs](https://vercel.com/docs)
- [Guides](https://vercel.com/guides)
- [Academy](https://vercel.com/academy)
- [Help](https://vercel.com/help)
- [Integrations](https://vercel.com/integrations)
- [Pricing](https://vercel.com/pricing)
- [Resources](https://vercel.com/resources)
- [Solution Partners](https://vercel.com/partners/solution-partners)
- [Startups](https://vercel.com/startups)
- [Templates](https://vercel.com/templates)
- SDKs by Vercel
  - [AI SDK](https://sdk.vercel.ai/)
  - [Workflow DevKit](https://useworkflow.dev/)
  - [Flags SDK](https://flags-sdk.dev/)
  - [Chat SDK](https://chat-sdk.dev/)
  - [Streamdown AI](https://streamdown.ai/)

## Company

- [About](https://vercel.com/about)
- [Blog](https://vercel.com/blog)
- [Careers](https://vercel.com/careers)
- [Changelog](https://vercel.com/changelog)
- [Contact Us](https://vercel.com/contact)
- [Customers](https://vercel.com/customers)
- [Events](https://vercel.com/events)
- [Partners](https://vercel.com/partners)
- [Shipped](https://vercel.com/shipped)
- [Privacy Policy](https://vercel.com/legal/privacy-policy)
- Legal

## Social

- [GitHub](https://github.com/vercel)
- [LinkedIn](https://linkedin.com/company/vercel)
- [Twitter](https://x.com/vercel)
- [YouTube](https://youtube.com/@VercelHQ)

[All systems normal.](https://vercel-status.com/) Select a display theme:systemlightdark

## Ask AI

What is Vercel?What can I deploy with Vercel?What is Fluid Compute?How much does Vercel cost?

Tip: You can open and close chat with `Ctrl`  `I`